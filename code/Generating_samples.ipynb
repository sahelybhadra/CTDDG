{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing RDKit and molvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "\n",
    "import rdkit\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Draw\n",
    "from rdkit import DataStructs\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import rdmolfiles\n",
    "from rdkit.Chem.Draw import IPythonConsole\n",
    "IPythonConsole.ipython_useSVG=True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from molvs import standardize_smiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import math\n",
    "import random\n",
    "import argparse\n",
    "import itertools\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from scipy import sparse\n",
    "from mxnet.gluon import nn\n",
    "from collections import Counter\n",
    "from mxnet.autograd import Function\n",
    "from mxnet.gluon.data import Dataset\n",
    "from mxnet import gluon, autograd, nd\n",
    "from mxnet.gluon.data import DataLoader\n",
    "from abc import ABCMeta, abstractmethod\n",
    "from mxnet.gluon.data.sampler import Sampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtaining all molecular properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoleculeSpec(object):\n",
    "\n",
    "    def __init__(self, file_name='/workspace/data/atom_types.txt'):\n",
    "        self.atom_types = []\n",
    "        self.atom_symbols = []\n",
    "        with open(file_name) as f:\n",
    "            for line in f:\n",
    "                atom_type_i = line.strip('\\n').split(',')\n",
    "                self.atom_types.append((atom_type_i[0], int(atom_type_i[1]), int(atom_type_i[2])))\n",
    "                if atom_type_i[0] not in self.atom_symbols:\n",
    "                    self.atom_symbols.append(atom_type_i[0])\n",
    "        self.bond_orders = [Chem.BondType.AROMATIC,\n",
    "                            Chem.BondType.SINGLE,\n",
    "                            Chem.BondType.DOUBLE,\n",
    "                            Chem.BondType.TRIPLE]\n",
    "        self.max_iter = 120\n",
    "\n",
    "    def get_atom_type(self, atom):\n",
    "        atom_symbol = atom.GetSymbol()\n",
    "        atom_charge = atom.GetFormalCharge()\n",
    "        atom_hs = atom.GetNumExplicitHs()\n",
    "        return self.atom_types.index((atom_symbol, atom_charge, atom_hs))\n",
    "\n",
    "    def get_bond_type(self, bond):\n",
    "        return self.bond_orders.index(bond.GetBondType())\n",
    "\n",
    "    def index_to_atom(self, idx):\n",
    "        atom_symbol, atom_charge, atom_hs = self.atom_types[idx]\n",
    "        a = Chem.Atom(atom_symbol)\n",
    "        a.SetFormalCharge(atom_charge)\n",
    "        a.SetNumExplicitHs(atom_hs)\n",
    "        return a\n",
    "\n",
    "    def index_to_bond(self, mol, begin_id, end_id, idx):\n",
    "        mol.AddBond(begin_id, end_id, self.bond_orders[idx])\n",
    "\n",
    "    @property\n",
    "    def num_atom_types(self):\n",
    "        return len(self.atom_types)\n",
    "\n",
    "    @property\n",
    "    def num_bond_types(self):\n",
    "        return len(self.bond_orders)\n",
    "\n",
    "_mol_spec = None\n",
    "\n",
    "def get_mol_spec():\n",
    "    global _mol_spec\n",
    "    if _mol_spec is None:\n",
    "        _mol_spec = MoleculeSpec()\n",
    "    return _mol_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining utility functions for data preprocessing and postprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_graph_from_smiles(smiles):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "\n",
    "    # build graph\n",
    "    atom_types, atom_ranks, bonds, bond_types = [], [], [], []\n",
    "    for a, r in zip(mol.GetAtoms(), Chem.CanonicalRankAtoms(mol)):\n",
    "        atom_types.append(get_mol_spec().get_atom_type(a))\n",
    "        atom_ranks.append(r)\n",
    "    for b in mol.GetBonds():\n",
    "        idx_1, idx_2, bt = b.GetBeginAtomIdx(), b.GetEndAtomIdx(), get_mol_spec().get_bond_type(b)\n",
    "        bonds.append([idx_1, idx_2])\n",
    "        bond_types.append(bt)\n",
    "\n",
    "    # build nx graph\n",
    "    graph = nx.Graph()\n",
    "    graph.add_nodes_from(range(len(atom_types)))\n",
    "    graph.add_edges_from(bonds)\n",
    "\n",
    "    return graph, atom_types, atom_ranks, bonds, bond_types\n",
    "\n",
    "\n",
    "def get_graph_from_smiles_list(smiles_list):\n",
    "    graph_list = []\n",
    "    for smiles in smiles_list:\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "\n",
    "        # build graph\n",
    "        atom_types, bonds, bond_types = [], [], []\n",
    "        for a in mol.GetAtoms():\n",
    "            atom_types.append(get_mol_spec().get_atom_type(a))\n",
    "        for b in mol.GetBonds():\n",
    "            idx_1, idx_2, bt = b.GetBeginAtomIdx(), b.GetEndAtomIdx(), get_mol_spec().get_bond_type(b)\n",
    "            bonds.append([idx_1, idx_2])\n",
    "            bond_types.append(bt)\n",
    "\n",
    "        X_0 = np.array(atom_types, dtype=np.int64)\n",
    "        A_0 = np.concatenate([np.array(bonds, dtype=np.int64),\n",
    "                              np.array(bond_types, dtype=np.int64)[:, np.newaxis]],\n",
    "                             axis=1)\n",
    "        graph_list.append([X_0, A_0])\n",
    "    return graph_list\n",
    "\n",
    "\n",
    "def traverse_graph(graph, atom_ranks, current_node=None, step_ids=None, p=0.9, log_p=0.0):\n",
    "    if current_node is None:\n",
    "        next_nodes = range(len(atom_ranks))\n",
    "        step_ids = [-1, ] * len(next_nodes)\n",
    "        next_node_ranks = atom_ranks\n",
    "    else:\n",
    "        next_nodes = graph.neighbors(current_node)  # get neighbor nodes\n",
    "        next_nodes = [n for n in next_nodes if step_ids[n] < 0] # filter visited nodes\n",
    "        next_node_ranks = [atom_ranks[n] for n in next_nodes] # get ranks for neighbors\n",
    "    next_nodes = [n for n, r in sorted(zip(next_nodes, next_node_ranks), key=lambda _x:_x[1])] # sort by rank\n",
    "\n",
    "    # iterate through neighbors\n",
    "    while len(next_nodes) > 0:\n",
    "        if len(next_nodes)==1:\n",
    "            next_node = next_nodes[0]\n",
    "        elif random.random() >= (1 - p):\n",
    "            next_node = next_nodes[0]\n",
    "            log_p += np.log(p)\n",
    "        else:\n",
    "            next_node = next_nodes[random.randint(1, len(next_nodes) - 1)]\n",
    "            log_p += np.log((1.0 - p) / (len(next_nodes) - 1))\n",
    "        step_ids[next_node] = max(step_ids) + 1\n",
    "        _, log_p = traverse_graph(graph, atom_ranks, next_node, step_ids, p, log_p)\n",
    "        next_nodes = [n for n in next_nodes if step_ids[n] < 0] # filter visited nodes\n",
    "\n",
    "    return step_ids, log_p\n",
    "\n",
    "\n",
    "def single_reorder(X_0, A_0, step_ids):\n",
    "    X_0, A_0 = np.copy(X_0), np.copy(A_0)\n",
    "\n",
    "    step_ids = np.array(step_ids, dtype=np.int64)\n",
    "\n",
    "    # sort by step_ids\n",
    "    sorted_ids = np.argsort(step_ids)\n",
    "    X_0 = X_0[sorted_ids]\n",
    "    A_0[:, 0], A_0[:, 1] = step_ids[A_0[:, 0]], step_ids[A_0[:, 1]]\n",
    "    max_b, min_b = np.amax(A_0[:, :2], axis=1), np.amin(A_0[:, :2], axis=1)\n",
    "    A_0 = A_0[np.lexsort([-min_b, max_b]), :]\n",
    "\n",
    "    # separate append and connect\n",
    "    max_b, min_b = np.amax(A_0[:, :2], axis=1), np.amin(A_0[:, :2], axis=1)\n",
    "    is_append = np.concatenate([np.array([True]), max_b[1:] > max_b[:-1]])\n",
    "    A_0 = np.concatenate([np.where(is_append[:, np.newaxis],\n",
    "                                 np.stack([min_b, max_b], axis=1),\n",
    "                                 np.stack([max_b, min_b], axis=1)),\n",
    "                        A_0[:, -1:]], axis=1)\n",
    "\n",
    "    return X_0, A_0\n",
    "\n",
    "\n",
    "def single_expand(X_0, A_0):\n",
    "    X_0, A_0 = np.copy(X_0), np.copy(A_0)\n",
    "\n",
    "    # expand X\n",
    "    is_append_iter = np.less(A_0[:, 0], A_0[:, 1]).astype(np.int64)\n",
    "    NX = np.cumsum(np.pad(is_append_iter, [[1, 0]], mode='constant', constant_values=1))\n",
    "    shift = np.cumsum(np.pad(NX, [[1, 0]], mode='constant')[:-1])\n",
    "    X_index = np.arange(NX.sum(), dtype=np.int64) - np.repeat(shift, NX)\n",
    "    X = X_0[X_index]\n",
    "\n",
    "    # expand A\n",
    "    _, A_index = np.tril_indices(A_0.shape[0])\n",
    "    A = A_0[A_index, :]\n",
    "    NA = np.arange(A_0.shape[0] + 1)\n",
    "\n",
    "    # get action\n",
    "    # action_type, atom_type, bond_type, append_pos, connect_pos\n",
    "    action_type = 1 - is_append_iter\n",
    "    atom_type = np.where(action_type == 0, X_0[A_0[:, 1]], 0)\n",
    "    bond_type = A_0[:, 2]\n",
    "    append_pos = np.where(action_type == 0, A_0[:, 0], 0)\n",
    "    connect_pos = np.where(action_type == 1, A_0[:, 1], 0)\n",
    "    actions = np.stack([action_type, atom_type, bond_type, append_pos, connect_pos],\n",
    "                       axis=1)\n",
    "    last_action = [[2, 0, 0, 0, 0]]\n",
    "    actions = np.append(actions, last_action, axis=0)\n",
    "\n",
    "    action_0 = np.array([X_0[0]], dtype=np.int64)\n",
    "\n",
    "    # }}}\n",
    "\n",
    "    # {{{ Get mask\n",
    "    last_atom_index = shift + NX - 1\n",
    "    last_atom_mask = np.zeros_like(X)\n",
    "    last_atom_mask[last_atom_index] = np.where(\n",
    "        np.pad(is_append_iter, [[1, 0]], mode='constant', constant_values=1) == 1,\n",
    "        np.ones_like(last_atom_index),\n",
    "        np.ones_like(last_atom_index) * 2)\n",
    "    # }}}\n",
    "\n",
    "    return action_0, X, NX, A, NA, actions, last_atom_mask\n",
    "\n",
    "\n",
    "def get_d(A, X):\n",
    "    _to_sparse = lambda _A, _X: sparse.coo_matrix((np.ones([_A.shape[0] * 2], dtype=np.int64),\n",
    "                                                   (np.concatenate([_A[:, 0], _A[:, 1]], axis=0),\n",
    "                                                    np.concatenate([_A[:, 1], _A[:, 0]], axis=0))),\n",
    "                                                  shape=[_X.shape[0], ] * 2)\n",
    "    A_sparse = _to_sparse(A, X)\n",
    "\n",
    "    d2 = A_sparse * A_sparse\n",
    "    d3 = d2 * A_sparse\n",
    "\n",
    "    # get D_2\n",
    "    D_2 = np.stack(d2.nonzero(), axis=1)\n",
    "    D_2 = D_2[D_2[:, 0] < D_2[:, 1], :]\n",
    "\n",
    "    # get D_3\n",
    "    D_3 = np.stack(d3.nonzero(), axis=1)\n",
    "    D_3 = D_3[D_3[:, 0] < D_3[:, 1], :]\n",
    "\n",
    "    # remove D_1 elements from D_3\n",
    "    D_3_sparse = _to_sparse(D_3, X)\n",
    "    D_3_sparse = D_3_sparse - D_3_sparse.multiply(A_sparse)\n",
    "    D_3 = np.stack(D_3_sparse.nonzero(), axis=1)\n",
    "    D_3 = D_3[D_3[:, 0] < D_3[:, 1], :]\n",
    "\n",
    "    return D_2, D_3\n",
    "\n",
    "\n",
    "def merge_single_0(X_0, A_0, NX_0, NA_0):\n",
    "    # shift_ids\n",
    "    cumsum = np.cumsum(np.pad(NX_0, [[1, 0]], mode='constant')[:-1])\n",
    "    A_0[:, :2] += np.stack([np.repeat(cumsum, NA_0), ] * 2, axis=1)\n",
    "\n",
    "    # get D\n",
    "    D_0_2, D_0_3 = get_d(A_0, X_0)\n",
    "\n",
    "    # split A\n",
    "    A_split = []\n",
    "    for i in range(get_mol_spec().num_bond_types):\n",
    "        A_i = A_0[A_0[:, 2] == i, :2]\n",
    "        A_split.append(A_i)\n",
    "    A_split.extend([D_0_2, D_0_3])\n",
    "    A_0 = A_split\n",
    "\n",
    "    # NX_rep\n",
    "    NX_rep_0 = np.repeat(np.arange(NX_0.shape[0]), NX_0)\n",
    "\n",
    "    return X_0, A_0, NX_0, NX_rep_0\n",
    "\n",
    "\n",
    "def merge_single(X, A,\n",
    "                 NX, NA,\n",
    "                 mol_ids, rep_ids, iw_ids,\n",
    "                 action_0, actions,\n",
    "                 last_append_mask,\n",
    "                 log_p):\n",
    "    X, A, NX, NX_rep = merge_single_0(X, A, NX, NA)\n",
    "    cumsum = np.cumsum(np.pad(NX, [[1, 0]], mode='constant')[:-1])\n",
    "    actions[:, -2] += cumsum * (actions[:, 0] == 0)\n",
    "    actions[:, -1] += cumsum * (actions[:, 0] == 1)\n",
    "    mol_ids_rep = np.repeat(mol_ids, NX)\n",
    "    rep_ids_rep = np.repeat(rep_ids, NX)\n",
    "\n",
    "    return X, A,\\\n",
    "           mol_ids_rep, rep_ids_rep, iw_ids,\\\n",
    "           last_append_mask,\\\n",
    "           NX, NX_rep,\\\n",
    "           action_0, actions, \\\n",
    "           log_p\n",
    "\n",
    "def process_single(smiles, k, p):\n",
    "    graph, atom_types, atom_ranks, bonds, bond_types = get_graph_from_smiles(smiles)\n",
    "\n",
    "    # original\n",
    "    X_0 = np.array(atom_types, dtype=np.int64)\n",
    "    A_0 = np.concatenate([np.array(bonds, dtype=np.int64),\n",
    "                          np.array(bond_types, dtype=np.int64)[:, np.newaxis]],\n",
    "                         axis=1)\n",
    "\n",
    "    X, A = [], []\n",
    "    NX, NA = [], []\n",
    "    mol_ids, rep_ids, iw_ids = [], [], []\n",
    "    action_0, actions = [], []\n",
    "    last_append_mask = []\n",
    "    log_p = []\n",
    "\n",
    "    # random sampling decoding route\n",
    "    for i in range(k):\n",
    "        step_ids_i, log_p_i = traverse_graph(graph, atom_ranks, p=p)\n",
    "        X_i, A_i = single_reorder(X_0, A_0, step_ids_i)\n",
    "        action_0_i, X_i, NX_i, A_i, NA_i, actions_i, last_atom_mask_i = single_expand(X_i, A_i)\n",
    "\n",
    "        # appends\n",
    "        X.append(X_i)\n",
    "        A.append(A_i)\n",
    "        NX.append(NX_i)\n",
    "        NA.append(NA_i)\n",
    "        action_0.append(action_0_i)\n",
    "        actions.append(actions_i)\n",
    "        last_append_mask.append(last_atom_mask_i)\n",
    "\n",
    "        mol_ids.append(np.zeros_like(NX_i, dtype=np.int64))\n",
    "        rep_ids.append(np.ones_like(NX_i, dtype=np.int64) * i)\n",
    "        iw_ids.append(np.ones_like(NX_i, dtype=np.int64) * i)\n",
    "\n",
    "        log_p.append(log_p_i)\n",
    "\n",
    "    # concatenate\n",
    "    X = np.concatenate(X, axis=0)\n",
    "    A = np.concatenate(A, axis = 0)\n",
    "    NX = np.concatenate(NX, axis = 0)\n",
    "    NA = np.concatenate(NA, axis = 0)\n",
    "    action_0 = np.concatenate(action_0, axis = 0)\n",
    "    actions = np.concatenate(actions, axis = 0)\n",
    "    last_append_mask = np.concatenate(last_append_mask, axis = 0)\n",
    "    mol_ids = np.concatenate(mol_ids, axis = 0)\n",
    "    rep_ids = np.concatenate(rep_ids, axis = 0)\n",
    "    iw_ids = np.concatenate(iw_ids, axis = 0)\n",
    "    log_p = np.array(log_p, dtype=np.float32)\n",
    "\n",
    "    return X, A, NX, NA, mol_ids, rep_ids, iw_ids, action_0, actions, last_append_mask, log_p\n",
    "\n",
    "\n",
    "# noinspection PyArgumentList\n",
    "def get_mol_from_graph(X, A, sanitize=True):\n",
    "    try:\n",
    "        mol = Chem.RWMol(Chem.Mol())\n",
    "\n",
    "        X, A = X.tolist(), A.tolist()\n",
    "        for i, atom_type in enumerate(X):\n",
    "            mol.AddAtom(get_mol_spec().index_to_atom(atom_type))\n",
    "\n",
    "        for atom_id1, atom_id2, bond_type in A:\n",
    "            get_mol_spec().index_to_bond(mol, atom_id1, atom_id2, bond_type)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "    if sanitize:\n",
    "        try:\n",
    "            mol = mol.GetMol()\n",
    "            Chem.SanitizeMol(mol)\n",
    "            return mol\n",
    "        except:\n",
    "            return None\n",
    "    else:\n",
    "        return mol\n",
    "\n",
    "def get_mol_from_graph_list(graph_list, sanitize=True):\n",
    "    mol_list = [get_mol_from_graph(X, A, sanitize) for X, A in graph_list]\n",
    "    return mol_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining graph convolution and other functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConvFn(Function):\n",
    "\n",
    "    def __init__(self, A):\n",
    "        self.A = A # type: nd.sparse.CSRNDArray\n",
    "        self.A_T = self.A # assume symmetric\n",
    "        super(GraphConvFn, self).__init__()\n",
    "\n",
    "    def forward(self, X):\n",
    "        if self.A is not None:\n",
    "            if len(X.shape) > 2:\n",
    "                X_resized = X.reshape((X.shape[0], -1))\n",
    "                output = nd.sparse.dot(self.A, X_resized)\n",
    "                output = output.reshape([-1, ] + [X.shape[i] for i in range(1, len(X.shape))])\n",
    "            else:\n",
    "                output = nd.sparse.dot(self.A, X)\n",
    "            return output\n",
    "        else:\n",
    "            return nd.zeros_like(X)\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "\n",
    "        if self.A is not None:\n",
    "            if len(grad_output.shape) > 2:\n",
    "                grad_output_resized = grad_output.reshape((grad_output.shape[0], -1))\n",
    "                grad_input = nd.sparse.dot(self.A_T, grad_output_resized)\n",
    "                grad_input = grad_input.reshape([-1] + [grad_output.shape[i]\n",
    "                                                        for i in range(1, len(grad_output.shape))])\n",
    "            else:\n",
    "                grad_input = nd.sparse.dot(self.A_T, grad_output)\n",
    "            return grad_input\n",
    "        else:\n",
    "            return nd.zeros_like(grad_output)\n",
    "\n",
    "\n",
    "class EfficientGraphConvFn(Function):\n",
    "    \"\"\"Save memory by re-computation\"\"\"\n",
    "\n",
    "    def __init__(self, A_list):\n",
    "        self.A_list = A_list\n",
    "        super(EfficientGraphConvFn, self).__init__()\n",
    "\n",
    "    def forward(self, X, W):\n",
    "        X_list = [X]\n",
    "        for A in self.A_list:\n",
    "            if A is not None:\n",
    "                X_list.append(nd.sparse.dot(A, X))\n",
    "            else:\n",
    "                X_list.append(nd.zeros_like(X))\n",
    "        X_out = nd.concat(*X_list, dim=1)\n",
    "        self.save_for_backward(X, W)\n",
    "\n",
    "        return nd.dot(X_out, W)\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        X, W = self.saved_tensors\n",
    "\n",
    "        # recompute X_out\n",
    "        X_list = [X, ]\n",
    "        for A in self.A_list:\n",
    "            if A is not None:\n",
    "                X_list.append(nd.sparse.dot(A, X))\n",
    "            else:\n",
    "                X_list.append(nd.zeros_like(X))\n",
    "        X_out = nd.concat(*X_list, dim=1)\n",
    "\n",
    "        grad_W = nd.dot(X_out.T, grad_output)\n",
    "\n",
    "        grad_X_out = nd.dot(grad_output, W.T)\n",
    "        grad_X_out_list = nd.split(grad_X_out, num_outputs=len(self.A_list) + 1)\n",
    "\n",
    "\n",
    "        grad_X = [grad_X_out_list[0], ]\n",
    "        for A, grad_X_out in zip(self.A_list, grad_X_out_list[1:]):\n",
    "            if A is not None:\n",
    "                grad_X.append(nd.sparse.dot(A, grad_X_out))\n",
    "            else:\n",
    "                grad_X.append(nd.zeros_like(grad_X_out))\n",
    "\n",
    "        grad_X = sum(grad_X)\n",
    "\n",
    "        return grad_X, grad_W\n",
    "\n",
    "\n",
    "class SegmentSumFn(GraphConvFn):\n",
    "\n",
    "    def __init__(self, idx, num_seg):\n",
    "        # build A\n",
    "        # construct coo\n",
    "        data = nd.ones(idx.shape[0], ctx=idx.context, dtype='int64')\n",
    "        row, col = idx, nd.arange(idx.shape[0], ctx=idx.context, dtype='int64')\n",
    "        shape = (num_seg, int(idx.shape[0]))\n",
    "        sparse = nd.sparse.csr_matrix((data, (row, col)), shape=shape,\n",
    "                                      ctx=idx.context, dtype='float32')\n",
    "        super(SegmentSumFn, self).__init__(sparse)\n",
    "\n",
    "        sparse = nd.sparse.csr_matrix((data, (col, row)), shape=(shape[1], shape[0]),\n",
    "                                      ctx=idx.context, dtype='float32')\n",
    "        self.A_T = sparse\n",
    "\n",
    "\n",
    "def squeeze(input, axis):\n",
    "    assert input.shape[axis] == 1\n",
    "\n",
    "    new_shape = list(input.shape)\n",
    "    del new_shape[axis]\n",
    "\n",
    "    return input.reshape(new_shape)\n",
    "\n",
    "\n",
    "def unsqueeze(input, axis):\n",
    "    return nd.expand_dims(input, axis=axis)\n",
    "\n",
    "\n",
    "def logsumexp(inputs, axis=None, keepdims=False):\n",
    "    \"\"\"Numerically stable logsumexp.\n",
    "    Args:\n",
    "        inputs: A Variable with any shape.\n",
    "        axis: An integer.\n",
    "        keepdims: A boolean.\n",
    "    Returns:\n",
    "        Equivalent of log(sum(exp(inputs), dim=dim, keepdim=keepdim)).\n",
    "    Adopted from: https://github.com/pytorch/pytorch/issues/2591\n",
    "    \"\"\"\n",
    "    # For a 1-D array x (any array along a single dimension),\n",
    "    # log sum exp(x) = s + log sum exp(x - s)\n",
    "    # with s = max(x) being a common choice.\n",
    "    if axis is None:\n",
    "        inputs = inputs.reshape([-1])\n",
    "        axis = 0\n",
    "    s = nd.max(inputs, axis=axis, keepdims=True)\n",
    "    outputs = s + (inputs - s).exp().sum(axis=axis, keepdims=True).log()\n",
    "    if not keepdims:\n",
    "        outputs = nd.sum(outputs, axis=axis, keepdims=False)\n",
    "    return outputs\n",
    "\n",
    "\n",
    "def get_activation(name):\n",
    "    activation_dict = {\n",
    "        'relu':nd.relu,\n",
    "        'tanh':nd.tanh\n",
    "    }\n",
    "    return activation_dict[name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear_BN(nn.Sequential):\n",
    "    def __init__(self, F_in, F_out):\n",
    "        super(Linear_BN, self).__init__()\n",
    "        self.add(nn.Dense(F_out, in_units=F_in, use_bias=False))\n",
    "        self.add(BatchNorm(in_channels=F_out))\n",
    "\n",
    "\n",
    "class GraphConv(nn.Block):\n",
    "\n",
    "    def __init__(self, Fin, Fout, D):\n",
    "        super(GraphConv, self).__init__()\n",
    "\n",
    "        # model settings\n",
    "        self.Fin = Fin\n",
    "        self.Fout = Fout\n",
    "        self.D = D\n",
    "\n",
    "        # model parameters\n",
    "        self.W = self.params.get('w', shape=(self.Fin * (self.D + 1), self.Fout),\n",
    "                                 init=None, allow_deferred_init=False)\n",
    "\n",
    "    def forward(self, X, A_list):\n",
    "        try:\n",
    "            assert len(A_list) == self.D\n",
    "        except AssertionError as e:\n",
    "            print(self.D, len(A_list))\n",
    "            raise e\n",
    "        return EfficientGraphConvFn(A_list)(X, self.W.data(X.context))\n",
    "\n",
    "\n",
    "class Policy(nn.Block):\n",
    "\n",
    "    def __init__(self, F_in, F_h, N_A, N_B, k=1):\n",
    "        super(Policy, self).__init__()\n",
    "        self.F_in = F_in # number of input features for each atom\n",
    "        self.F_h = F_h # number of context variables\n",
    "        self.N_A = N_A # number of atom types\n",
    "        self.N_B = N_B # number of bond types\n",
    "        self.k = k # number of softmax used in the mixture\n",
    "\n",
    "\n",
    "        with self.name_scope():\n",
    "            self.linear_h = Linear_BN(F_in * 2, self.F_h * k)\n",
    "            self.linear_h_t = Linear_BN(F_in, self.F_h * k)\n",
    "\n",
    "            self.linear_x = nn.Dense(self.N_B + self.N_B*self.N_A, in_units=self.F_h)\n",
    "            self.linear_x_t = nn.Dense(1, in_units=self.F_h)\n",
    "\n",
    "            if self.k > 1:\n",
    "                self.linear_pi = nn.Dense(self.k, in_units=self.F_in)\n",
    "            else:\n",
    "                self.linear_pi = None\n",
    "\n",
    "    def forward(self, X, NX, NX_rep, X_end=None):\n",
    "        # segment mean for X\n",
    "        if X_end is None:\n",
    "            X_end = SegmentSumFn(NX_rep, NX.shape[0])(X)/nd.cast(fn.unsqueeze(NX, 1), 'float32')\n",
    "        X = nd.concat(X, X_end[NX_rep, :], dim=1)\n",
    "\n",
    "        X_h = nd.relu(self.linear_h(X)).reshape([-1, self.F_h])\n",
    "        X_h_end = nd.relu(self.linear_h_t(X_end)).reshape([-1, self.F_h])\n",
    "\n",
    "        X_x = nd.exp(self.linear_x(X_h)).reshape([-1, self.k, self.N_B + self.N_B*self.N_A])\n",
    "        X_x_end = nd.exp(self.linear_x_t(X_h_end)).reshape([-1, self.k, 1])\n",
    "\n",
    "        X_sum = nd.sum(SegmentSumFn(NX_rep, NX.shape[0])(X_x), -1, keepdims=True) + X_x_end\n",
    "        X_sum_gathered = X_sum[NX_rep, :, :]\n",
    "\n",
    "        X_softmax = X_x / X_sum_gathered\n",
    "        X_softmax_end = X_x_end/ X_sum\n",
    "\n",
    "        if self.k > 1:\n",
    "            pi = unsqueeze(nd.softmax(self.linear_pi(X_end), axis=1), -1)\n",
    "            pi_gathered = pi[NX_rep, :, :]\n",
    "\n",
    "            X_softmax = nd.sum(X_softmax * pi_gathered, axis=1)\n",
    "            X_softmax_end = nd.sum(X_softmax_end * pi, axis=1)\n",
    "        else:\n",
    "            X_softmax = squeeze(X_softmax, 1)\n",
    "            X_softmax_end = squeeze(X_softmax_end, 1)\n",
    "\n",
    "        # generate output probabilities\n",
    "        connect, append = X_softmax[:, :self.N_B], X_softmax[:, self.N_B:]\n",
    "        append = append.reshape([-1, self.N_A, self.N_B])\n",
    "        end = squeeze(X_softmax_end, -1)\n",
    "\n",
    "        return append, connect, end\n",
    "\n",
    "\n",
    "class BatchNorm(nn.Block):\n",
    "\n",
    "    def __init__(self, in_channels, momentum=0.9, eps=1e-5):\n",
    "        super(BatchNorm, self).__init__()\n",
    "        self.F = in_channels\n",
    "\n",
    "        self.bn_weight = self.params.get('bn_weight', shape=(self.F,), init=mx.init.One(),\n",
    "                                         allow_deferred_init=False)\n",
    "        self.bn_bias = self.params.get('bn_bias', shape=(self.F,), init=mx.init.Zero(),\n",
    "                                       allow_deferred_init=False)\n",
    "\n",
    "        self.running_mean = self.params.get('running_mean', grad_req='null',\n",
    "                                            shape=(self.F,),\n",
    "                                            init=mx.init.Zero(),\n",
    "                                            allow_deferred_init=False,\n",
    "                                            differentiable=False)\n",
    "        self.running_var = self.params.get('running_var', grad_req='null',\n",
    "                                           shape=(self.F,),\n",
    "                                           init=mx.init.One(),\n",
    "                                           allow_deferred_init=False,\n",
    "                                           differentiable=False)\n",
    "        self.momentum = momentum\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        if autograd.is_training():\n",
    "            return nd.BatchNorm(x,\n",
    "                                gamma=self.bn_weight.data(x.context),\n",
    "                                beta=self.bn_bias.data(x.context),\n",
    "                                moving_mean=self.running_mean.data(x.context),\n",
    "                                moving_var=self.running_var.data(x.context),\n",
    "                                eps=self.eps, momentum=self.momentum,\n",
    "                                use_global_stats=False)\n",
    "        else:\n",
    "            return nd.BatchNorm(x,\n",
    "                                gamma=self.bn_weight.data(x.context),\n",
    "                                beta=self.bn_bias.data(x.context),\n",
    "                                moving_mean=self.running_mean.data(x.context),\n",
    "                                moving_var=self.running_var.data(x.context),\n",
    "                                eps=self.eps, momentum=self.momentum,\n",
    "                                use_global_stats=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building generative network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoleculeGenerator(nn.Block):\n",
    "\n",
    "    __metaclass__ = ABCMeta\n",
    "\n",
    "    def __init__(self, N_A, N_B, D, F_e, F_skip, F_c, Fh_policy, activation,\n",
    "                 *args, **kwargs):\n",
    "        super(MoleculeGenerator, self).__init__()\n",
    "        self.N_A = N_A\n",
    "        self.N_B = N_B\n",
    "        self.D = D\n",
    "        self.F_e = F_e\n",
    "        self.F_skip = F_skip\n",
    "        self.F_c = list(F_c) if isinstance(F_c, tuple) else F_c\n",
    "        self.Fh_policy = Fh_policy\n",
    "        self.activation = get_activation(activation)\n",
    "\n",
    "        with self.name_scope():\n",
    "            # embeddings\n",
    "            self.embedding_atom = nn.Embedding(self.N_A, self.F_e)\n",
    "            self.embedding_mask = nn.Embedding(3, self.F_e)\n",
    "\n",
    "            # graph conv\n",
    "            self._build_graph_conv(*args, **kwargs)\n",
    "\n",
    "            # fully connected\n",
    "            self.dense = nn.Sequential()\n",
    "            for i, (f_in, f_out) in enumerate(zip([self.F_skip, ] + self.F_c[:-1], self.F_c)):\n",
    "                self.dense.add(Linear_BN(f_in, f_out))\n",
    "\n",
    "            # policy\n",
    "            self.policy_0 = self.params.get('policy_0', shape=[self.N_A, ],\n",
    "                                            init=mx.init.Zero(),\n",
    "                                            allow_deferred_init=False)\n",
    "            self.policy_h = Policy(self.F_c[-1], self.Fh_policy, self.N_A, self.N_B)\n",
    "\n",
    "        self.mode = 'loss'\n",
    "\n",
    "    @abstractmethod\n",
    "    def _build_graph_conv(self, *args, **kwargs):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @abstractmethod\n",
    "    def _graph_conv_forward(self, X, A):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _policy_0(self, ctx):\n",
    "        policy_0 = nd.exp(self.policy_0.data(ctx))\n",
    "        policy_0 = policy_0/policy_0.sum()\n",
    "        return policy_0\n",
    "\n",
    "    def _policy(self, X, A, NX, NX_rep, last_append_mask):\n",
    "        # get initial embedding\n",
    "        X = self.embedding_atom(X) + self.embedding_mask(last_append_mask)\n",
    "\n",
    "        # convolution\n",
    "        X = self._graph_conv_forward(X, A)\n",
    "\n",
    "        # linear\n",
    "        X = self.dense(X)\n",
    "\n",
    "        # policy\n",
    "        append, connect, end = self.policy_h(X, NX, NX_rep)\n",
    "\n",
    "        return append, connect, end\n",
    "\n",
    "    def _likelihood(self, init, append, connect, end,\n",
    "                    action_0, actions, iw_ids, log_p_sigma,\n",
    "                    batch_size, iw_size):\n",
    "\n",
    "        # decompose action:\n",
    "        action_type, node_type, edge_type, append_pos, connect_pos = \\\n",
    "            actions[:, 0], actions[:, 1], actions[:, 2], actions[:, 3], actions[:, 4]\n",
    "        _log_mask = lambda _x, _mask: _mask * nd.log(_x + 1e-10) + (1- _mask) * nd.zeros_like(_x)\n",
    "\n",
    "        # init\n",
    "        init = init.reshape([batch_size * iw_size, self.N_A])\n",
    "        index = nd.stack(nd.arange(action_0.shape[0], ctx=action_0.context, dtype='int64'), action_0, axis=0)\n",
    "        loss_init = nd.log(nd.gather_nd(init, index) + 1e-10)\n",
    "\n",
    "        # end\n",
    "        loss_end = _log_mask(end, nd.cast(action_type == 2, 'float32'))\n",
    "\n",
    "        # append\n",
    "        index = nd.stack(append_pos, node_type, edge_type, axis=0)\n",
    "        loss_append = _log_mask(nd.gather_nd(append, index), nd.cast(action_type == 0, 'float32'))\n",
    "\n",
    "        # connect\n",
    "        index = nd.stack(connect_pos, edge_type, axis=0)\n",
    "        loss_connect = _log_mask(nd.gather_nd(connect, index), nd.cast(action_type == 1, 'float32'))\n",
    "\n",
    "        # sum up results\n",
    "        log_p_x = loss_end + loss_append + loss_connect\n",
    "        log_p_x = squeeze(SegmentSumFn(iw_ids, batch_size*iw_size)(unsqueeze(log_p_x, -1)), -1)\n",
    "        log_p_x = log_p_x + loss_init\n",
    "\n",
    "        # reshape\n",
    "        log_p_x = log_p_x.reshape([batch_size, iw_size])\n",
    "        log_p_sigma = log_p_sigma.reshape([batch_size, iw_size])\n",
    "        l = log_p_x - log_p_sigma\n",
    "        l = logsumexp(l, axis=1) - math.log(float(iw_size))\n",
    "        return l\n",
    "\n",
    "    def forward(self, *input):\n",
    "        if self.mode=='loss' or self.mode=='likelihood':\n",
    "            X, A, iw_ids, last_append_mask, \\\n",
    "            NX, NX_rep, action_0, actions, log_p, \\\n",
    "            batch_size, iw_size = input\n",
    "\n",
    "            init = self._policy_0(X.context).tile([batch_size * iw_size, 1])\n",
    "            append, connect, end = self._policy(X, A, NX, NX_rep, last_append_mask)\n",
    "            l = self._likelihood(init, append, connect, end, action_0, actions, iw_ids, log_p, batch_size, iw_size)\n",
    "            if self.mode=='likelihood':\n",
    "                return l\n",
    "            else:\n",
    "                return -l.mean()\n",
    "        elif self.mode == 'decode_0':\n",
    "            return self._policy_0(input[0])\n",
    "        elif self.mode == 'decode_step':\n",
    "            X, A, NX, NX_rep, last_append_mask = input\n",
    "            return self._policy(X, A, NX, NX_rep, last_append_mask)\n",
    "\n",
    "\n",
    "class MoleculeGenerator_RNN(MoleculeGenerator):\n",
    "\n",
    "    __metaclass__ = ABCMeta\n",
    "\n",
    "    def __init__(self, N_A, N_B, D, F_e, F_skip, F_c, Fh_policy, activation,\n",
    "                 N_rnn, *args, **kwargs):\n",
    "        super(MoleculeGenerator_RNN, self).__init__(N_A, N_B, D, F_e, F_skip, F_c, Fh_policy, activation,\n",
    "                                                    *args, **kwargs)\n",
    "        self.N_rnn = N_rnn\n",
    "\n",
    "        with self.name_scope():\n",
    "            self.rnn = gluon.rnn.GRU(hidden_size=self.F_c[-1],\n",
    "                                     num_layers=self.N_rnn,\n",
    "                                     layout='NTC', input_size=self.F_c[-1] * 2)\n",
    "\n",
    "    def _rnn_train(self, X, NX, NX_rep, graph_to_rnn, rnn_to_graph, NX_cum):\n",
    "        X_avg = SegmentSumFn(NX_rep, NX.shape[0])(X) / nd.cast(unsqueeze(NX, 1), 'float32')\n",
    "        X_curr = nd.take(X, indices=NX_cum-1)\n",
    "        X = nd.concat(X_avg, X_curr, dim=1)\n",
    "\n",
    "        # rnn\n",
    "        X = nd.take(X, indices=graph_to_rnn) # batch_size, iw_size, length, num_features\n",
    "        batch_size, iw_size, length, num_features = X.shape\n",
    "        X = X.reshape([batch_size*iw_size, length, num_features])\n",
    "        X = self.rnn(X)\n",
    "\n",
    "        X = X.reshape([batch_size, iw_size, length, -1])\n",
    "        X = nd.gather_nd(X, indices=rnn_to_graph)\n",
    "\n",
    "        return X\n",
    "\n",
    "    def _rnn_test(self, X, NX, NX_rep, NX_cum, h):\n",
    "        # note: one partition for one molecule\n",
    "        X_avg = SegmentSumFn(NX_rep, NX.shape[0])(X) / nd.cast(unsqueeze(NX, 1), 'float32')\n",
    "        X_curr = nd.take(X, indices=NX_cum - 1)\n",
    "        X = nd.concat(X_avg, X_curr, dim=1) # size: [NX, F_in * 2]\n",
    "\n",
    "        # rnn\n",
    "        X = unsqueeze(X, axis=1)\n",
    "        X, h = self.rnn(X, h)\n",
    "\n",
    "        X = squeeze(X, axis=1)\n",
    "        return X, h\n",
    "\n",
    "    def _policy(self, X, A, NX, NX_rep, last_append_mask, graph_to_rnn, rnn_to_graph, NX_cum):\n",
    "        # get initial embedding\n",
    "        X = self.embedding_atom(X) + self.embedding_mask(last_append_mask)\n",
    "\n",
    "        # convolution\n",
    "        X = self._graph_conv_forward(X, A)\n",
    "\n",
    "        # linear\n",
    "        X = self.dense(X)\n",
    "\n",
    "        # rnn\n",
    "        X_mol = self._rnn_train(X, NX, NX_rep, graph_to_rnn, rnn_to_graph, NX_cum)\n",
    "\n",
    "        # policy\n",
    "        append, connect, end = self.policy_h(X, NX, NX_rep, X_mol)\n",
    "\n",
    "        return append, connect, end\n",
    "\n",
    "    def _decode_step(self, X, A, NX, NX_rep, last_append_mask, NX_cum, h):\n",
    "        # get initial embedding\n",
    "        X = self.embedding_atom(X) + self.embedding_mask(last_append_mask)\n",
    "\n",
    "        # convolution\n",
    "        X = self._graph_conv_forward(X, A)\n",
    "\n",
    "        # linear\n",
    "        X = self.dense(X)\n",
    "\n",
    "        # rnn\n",
    "        X_mol, h = self._rnn_test(X, NX, NX_rep, NX_cum, h)\n",
    "\n",
    "        # policy\n",
    "        append, connect, end = self.policy_h(X, NX, NX_rep, X_mol)\n",
    "\n",
    "        return append, connect, end, h\n",
    "\n",
    "    def forward(self, *input):\n",
    "        if self.mode=='loss' or self.mode=='likelihood':\n",
    "            X, A, iw_ids, last_append_mask, \\\n",
    "            NX, NX_rep, action_0, actions, log_p, \\\n",
    "            batch_size, iw_size, \\\n",
    "            graph_to_rnn, rnn_to_graph, NX_cum = input\n",
    "\n",
    "            init = self._policy_0(X.context).tile([batch_size * iw_size, 1])\n",
    "            append, connect, end = self._policy(X, A, NX, NX_rep, last_append_mask, graph_to_rnn, rnn_to_graph, NX_cum)\n",
    "            l = self._likelihood(init, append, connect, end, action_0, actions, iw_ids, log_p, batch_size, iw_size)\n",
    "            if self.mode=='likelihood':\n",
    "                return l\n",
    "            else:\n",
    "                return -l.mean()\n",
    "        elif self.mode == 'decode_0':\n",
    "            return self._policy_0(input[0])\n",
    "        elif self.mode == 'decode_step':\n",
    "            X, A, NX, NX_rep, last_append_mask, NX_cum, h = input\n",
    "            return self._decode_step(X, A, NX, NX_rep, last_append_mask, NX_cum, h)\n",
    "        else:\n",
    "            raise ValueError\n",
    "            \n",
    "class _TwoLayerDense(nn.Block):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(_TwoLayerDense, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.input_size = input_size\n",
    "\n",
    "        with self.name_scope():\n",
    "            # config 1\n",
    "            self.input = nn.Dense(self.hidden_size, use_bias=False, in_units=self.input_size)\n",
    "            self.bn_input = BatchNorm(in_channels=self.hidden_size)\n",
    "            self.output = nn.Dense(self.output_size, use_bias=True, in_units=self.hidden_size)\n",
    "            \n",
    "            # config 2\n",
    "            #self.output = nn.Dense(self.output_size, use_bias=True, in_units=self.input_size)\n",
    "            \n",
    "            # config 3\n",
    "            #self.input1 = nn.Dense(self.hidden_size, use_bias=False, in_units=self.input_size)\n",
    "            #self.bn_input1 = BatchNorm(in_channels=self.hidden_size)\n",
    "            #self.input2 = nn.Dense(self.hidden_size, use_bias=False, in_units=self.hidden_size)\n",
    "            #self.bn_input2 = BatchNorm(in_channels=self.hidden_size)\n",
    "            #self.output = nn.Dense(self.output_size, use_bias=True, in_units=self.hidden_size)\n",
    "\n",
    "    def forward(self, c):\n",
    "        # config 1\n",
    "        return nd.softmax(self.output(nd.relu(self.bn_input(self.input(c)))), axis=-1)\n",
    "        \n",
    "        # config 2\n",
    "        #return nd.softmax(self.output(c), axis=-1)\n",
    "    \n",
    "        # config 3\n",
    "        #return nd.softmax(self.output(nd.relu(self.bn_input2(self.input2(nd.relu(self.bn_input1(self.input1(c))))))), axis=-1)\n",
    "\n",
    "\n",
    "class CMoleculeGenerator_RNN(MoleculeGenerator_RNN):\n",
    "    __metaclass__ = ABCMeta\n",
    "\n",
    "    def __init__(self, N_A, N_B, N_C, D,\n",
    "                 F_e, F_skip, F_c, Fh_policy,\n",
    "                 activation, N_rnn,\n",
    "                 *args, **kwargs):\n",
    "        self.N_C = N_C # number of conditional variables\n",
    "        super(CMoleculeGenerator_RNN, self).__init__(N_A, N_B, D,\n",
    "                                                     F_e, F_skip, F_c, Fh_policy,\n",
    "                                                     activation, N_rnn,\n",
    "                                                     *args, **kwargs)\n",
    "        with self.name_scope():\n",
    "            self.dense_policy_0 = _TwoLayerDense(self.N_C, self.N_A * 3, self.N_A)\n",
    "\n",
    "    @abstractmethod\n",
    "    def _graph_conv_forward(self, X, A, c, ids):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _policy_0(self, c):\n",
    "        return self.dense_policy_0(c) + 0.0 * self.policy_0.data(c.context)\n",
    "\n",
    "    def _policy(self, X, A, NX, NX_rep, last_append_mask,\n",
    "                graph_to_rnn, rnn_to_graph, NX_cum,\n",
    "                c, ids):\n",
    "        # get initial embedding\n",
    "        X = self.embedding_atom(X) + self.embedding_mask(last_append_mask)\n",
    "\n",
    "        # convolution\n",
    "        X = self._graph_conv_forward(X, A, c, ids)\n",
    "\n",
    "        # linear\n",
    "        X = self.dense(X)\n",
    "\n",
    "        # rnn\n",
    "        X_mol = self._rnn_train(X, NX, NX_rep, graph_to_rnn, rnn_to_graph, NX_cum)\n",
    "\n",
    "        # policy\n",
    "        append, connect, end = self.policy_h(X, NX, NX_rep, X_mol)\n",
    "\n",
    "        return append, connect, end\n",
    "\n",
    "    def _decode_step(self, X, A, NX, NX_rep, last_append_mask, NX_cum, h, c, ids):\n",
    "        # get initial embedding\n",
    "        X = self.embedding_atom(X) + self.embedding_mask(last_append_mask)\n",
    "\n",
    "        # convolution\n",
    "        X = self._graph_conv_forward(X, A, c, ids)\n",
    "\n",
    "        # linear\n",
    "        X = self.dense(X)\n",
    "\n",
    "        # rnn\n",
    "        X_mol, h = self._rnn_test(X, NX, NX_rep, NX_cum, h)\n",
    "\n",
    "        # policy\n",
    "        append, connect, end = self.policy_h(X, NX, NX_rep, X_mol)\n",
    "\n",
    "        return append, connect, end, h\n",
    "\n",
    "\n",
    "    def forward(self, *input):\n",
    "        if self.mode=='loss' or self.mode=='likelihood':\n",
    "            X, A, iw_ids, last_append_mask, \\\n",
    "            NX, NX_rep, action_0, actions, log_p, \\\n",
    "            batch_size, iw_size, \\\n",
    "            graph_to_rnn, rnn_to_graph, NX_cum, \\\n",
    "            c, ids = input\n",
    "\n",
    "            init = nd.tile(unsqueeze(self._policy_0(c), axis=1), [1, iw_size, 1])\n",
    "            append, connect, end = self._policy(X, A, NX, NX_rep, last_append_mask,\n",
    "                                                graph_to_rnn, rnn_to_graph, NX_cum,\n",
    "                                                c, ids)\n",
    "            l = self._likelihood(init, append, connect, end,\n",
    "                                 action_0, actions, iw_ids, log_p,\n",
    "                                 batch_size, iw_size)\n",
    "            if self.mode=='likelihood':\n",
    "                return l\n",
    "            else:\n",
    "                return -l.mean()\n",
    "        elif self.mode == 'decode_0':\n",
    "            return self._policy_0(*input)\n",
    "        elif self.mode == 'decode_step':\n",
    "            X, A, NX, NX_rep, last_append_mask, NX_cum, h, c, ids = input\n",
    "            return self._decode_step(X, A, NX, NX_rep, last_append_mask, NX_cum, h, c, ids)\n",
    "        else:\n",
    "            raise ValueError\n",
    "\n",
    "class CVanillaMolGen_RNN(CMoleculeGenerator_RNN):\n",
    "\n",
    "    def __init__(self, N_A, N_B, N_C, D,\n",
    "                 F_e, F_h, F_skip, F_c, Fh_policy,\n",
    "                 activation, N_rnn, rename=False):\n",
    "        self.rename = rename\n",
    "        super(CVanillaMolGen_RNN, self).__init__(N_A, N_B, N_C, D,\n",
    "                                                 F_e, F_skip, F_c, Fh_policy,\n",
    "                                                 activation, N_rnn,\n",
    "                                                 F_h)\n",
    "\n",
    "    def _build_graph_conv(self, F_h):\n",
    "        self.F_h = list(F_h) if isinstance(F_h, tuple) else F_h\n",
    "        self.conv, self.bn = [], []\n",
    "        for i, (f_in, f_out) in enumerate(zip([self.F_e] + self.F_h[:-1], self.F_h)):\n",
    "            conv = GraphConv(f_in, f_out, self.N_B + self.D)\n",
    "            self.conv.append(conv)\n",
    "            self.register_child(conv)\n",
    "\n",
    "            if i != 0:\n",
    "                bn = BatchNorm(in_channels=f_in)\n",
    "                self.register_child(bn)\n",
    "            else:\n",
    "                bn = None\n",
    "            self.bn.append(bn)\n",
    "\n",
    "        self.bn_skip = BatchNorm(in_channels=sum(self.F_h))\n",
    "        self.linear_skip = Linear_BN(sum(self.F_h), self.F_skip)\n",
    "\n",
    "        # projectors for conditional variable (protein embedding)\n",
    "        self.linear_c = []\n",
    "        for i, f_out in enumerate(self.F_h):\n",
    "            if self.rename:\n",
    "                linear_c = nn.Dense(f_out, use_bias=False, in_units=self.N_C, prefix='cond_{}'.format(i))\n",
    "            else:\n",
    "                linear_c = nn.Dense(f_out, use_bias=False, in_units=self.N_C)\n",
    "            self.register_child(linear_c)\n",
    "            self.linear_c.append(linear_c)\n",
    "\n",
    "    def _graph_conv_forward(self, X, A, c, ids):\n",
    "        X_out = [X]\n",
    "        for conv, bn, linear_c in zip(self.conv, self.bn, self.linear_c):\n",
    "            X = X_out[-1]\n",
    "            if bn is not None:\n",
    "                X_out.append(conv(self.activation(bn(X)), A) + linear_c(c)[ids, :])\n",
    "            else:\n",
    "                X_out.append(conv(X, A) + linear_c(c)[ids, :])\n",
    "        X_out = nd.concat(*X_out[1:], dim=1)\n",
    "        return self.activation(self.linear_skip(self.activation(self.bn_skip(X_out))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining functions for generating samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _decode_step(X, A, NX, NA, last_action, finished,\n",
    "                 get_init, get_action,\n",
    "                 random=False, n_node_types=get_mol_spec().num_atom_types,\n",
    "                 n_edge_types=get_mol_spec().num_bond_types):\n",
    "    if X is None:\n",
    "        init = get_init()\n",
    "\n",
    "        if random:\n",
    "            X = []\n",
    "            for i in range(init.shape[0]):\n",
    "                p = init[i, :]\n",
    "                selected_atom = np.random.choice(np.arange(init.shape[1]), 1, p=p)[0]\n",
    "                X.append(selected_atom)\n",
    "            X = np.array(X, dtype=np.int64)\n",
    "        else:\n",
    "            X = np.argmax(init, axis=1)\n",
    "        A = np.zeros((0, 3), dtype=np.int64)\n",
    "        NX = last_action = np.ones([X.shape[0]], dtype=np.int64)\n",
    "        NA = np.zeros([X.shape[0]], dtype=np.int64)\n",
    "        finished = np.array([False, ] * X.shape[0], dtype=np.bool)\n",
    "\n",
    "        return X, A, NX, NA, last_action, finished\n",
    "    else:\n",
    "        X_u = X[np.repeat(np.logical_not(finished), NX)]\n",
    "        A_u = A[np.repeat(np.logical_not(finished), NA), :]\n",
    "        NX_u = NX[np.logical_not(finished)]\n",
    "        NA_u = NA[np.logical_not(finished)]\n",
    "        last_action_u = last_action[np.logical_not(finished)]\n",
    "\n",
    "        # conv\n",
    "        mol_ids_rep = NX_rep = np.repeat(np.arange(NX_u.shape[0]), NX_u)\n",
    "        rep_ids_rep = np.zeros_like(mol_ids_rep)\n",
    "\n",
    "        if A.shape[0] == 0:\n",
    "            D_2 = D_3 = np.zeros((0, 2), dtype=np.int64)\n",
    "            A_u = [np.zeros((0, 2), dtype=np.int64) for _ in range(get_mol_spec().num_bond_types)]\n",
    "            A_u += [D_2, D_3]\n",
    "        else:\n",
    "            cumsum = np.cumsum(np.pad(NX_u, [[1, 0]], mode='constant')[:-1])\n",
    "            shift = np.repeat(cumsum, NA_u)\n",
    "            A_u[:, :2] += np.stack([shift, ] * 2, axis=1)\n",
    "            D_2, D_3 = get_d(A_u, X_u)\n",
    "            A_u = [A_u[A_u[:, 2] == _i, :2] for _i in range(n_edge_types)]\n",
    "            A_u += [D_2, D_3]\n",
    "\n",
    "        mask = np.zeros([X_u.shape[0]], dtype=np.int64)\n",
    "        last_append_index = np.cumsum(NX_u) - 1\n",
    "        mask[last_append_index] = np.where(last_action_u == 1,\n",
    "                                           np.ones_like(last_append_index, dtype=np.int64),\n",
    "                                           np.ones_like(last_append_index, dtype=np.int64) * 2)\n",
    "\n",
    "        decode_input = [X_u, A_u, NX_u, NX_rep, mask, mol_ids_rep, rep_ids_rep]\n",
    "        append, connect, end = get_action(decode_input)\n",
    "\n",
    "        if A.shape[0] == 0:\n",
    "            max_index = np.argmax(np.reshape(append, [-1, n_node_types * n_edge_types]), axis=1)\n",
    "            atom_type, bond_type = np.unravel_index(max_index, [n_node_types, n_edge_types])\n",
    "            X = np.reshape(np.stack([X, atom_type], axis=1), [-1])\n",
    "            NX = np.array([2, ] * len(finished), dtype=np.int64)\n",
    "            A = np.stack([np.zeros([len(finished), ], dtype=np.int64),\n",
    "                          np.ones([len(finished), ], dtype=np.int64),\n",
    "                          bond_type], axis=1)\n",
    "            NA = np.ones([len(finished), ], dtype=np.int64)\n",
    "            last_action = np.ones_like(NX, dtype=np.int64)\n",
    "\n",
    "        else:\n",
    "            # process for each molecule\n",
    "            append, connect = np.split(append, np.cumsum(NX_u)), np.split(connect, np.cumsum(NX_u))\n",
    "            end = end.tolist()\n",
    "\n",
    "            unfinished_ids = np.where(np.logical_not(finished))[0].tolist()\n",
    "            cumsum = np.cumsum(NX)\n",
    "            cumsum_a = np.cumsum(NA)\n",
    "\n",
    "            X_insert = []\n",
    "            X_insert_ids = []\n",
    "            A_insert = []\n",
    "            A_insert_ids = []\n",
    "            finished_ids = []\n",
    "\n",
    "            for i, (unfinished_id, append_i, connect_i, end_i) \\\n",
    "                    in enumerate(zip(unfinished_ids, append, connect, end)):\n",
    "                if random:\n",
    "                    def _rand_id(*_x):\n",
    "                        _x_reshaped = [np.reshape(_xi, [-1]) for _xi in _x]\n",
    "                        _x_length = np.array([_x_reshape_i.shape[0] for _x_reshape_i in _x_reshaped],\n",
    "                                             dtype=np.int64)\n",
    "                        _begin = np.cumsum(np.pad(_x_length, [[1, 0]], mode='constant')[:-1])\n",
    "                        _end = np.cumsum(_x_length) - 1\n",
    "                        _p = np.concatenate(_x_reshaped)\n",
    "                        _p = _p / np.sum(_p)\n",
    "                        _rand_index = np.random.choice(np.arange(_p.shape[0]), 1, p=_p)[0]\n",
    "                        _p_step = _p[_rand_index]\n",
    "                        _x_index = np.where(np.logical_and(_begin <= _rand_index, _end >= _rand_index))[0][0]\n",
    "                        _rand_index = _rand_index - _begin[_x_index]\n",
    "                        _rand_index = np.unravel_index(_rand_index, _x[_x_index].shape)\n",
    "                        return _x_index, _rand_index, _p_step\n",
    "\n",
    "                    action_type, action_index, p_step = _rand_id(append_i, connect_i, np.array([end_i]))\n",
    "                else:\n",
    "                    _argmax = lambda _x: np.unravel_index(np.argmax(_x), _x.shape)\n",
    "                    append_id, append_val = _argmax(append_i), np.max(append_i)\n",
    "                    connect_id, connect_val = _argmax(connect_i), np.max(connect_i)\n",
    "                    end_val = end_i\n",
    "                    if end_val >= append_val and end_val >= connect_val:\n",
    "                        action_type = 2\n",
    "                        action_index = None\n",
    "                    elif append_val >= connect_val and append_val >= end_val:\n",
    "                        action_type = 0\n",
    "                        action_index = append_id\n",
    "                    else:\n",
    "                        action_type = 1\n",
    "                        action_index = connect_id\n",
    "                if action_type == 2:\n",
    "                    # finish growth\n",
    "                    finished_ids.append(unfinished_id)\n",
    "                elif action_type == 0:\n",
    "                    # append action\n",
    "                    append_pos, atom_type, bond_type = action_index\n",
    "                    X_insert.append(atom_type)\n",
    "                    X_insert_ids.append(unfinished_id)\n",
    "                    A_insert.append([append_pos, NX[unfinished_id], bond_type])\n",
    "                    A_insert_ids.append(unfinished_id)\n",
    "                else:\n",
    "                    # connect\n",
    "                    connect_ps, bond_type = action_index\n",
    "                    A_insert.append([NX[unfinished_id] - 1, connect_ps, bond_type])\n",
    "                    A_insert_ids.append(unfinished_id)\n",
    "            if len(A_insert_ids) > 0:\n",
    "                A = np.insert(A, cumsum_a[A_insert_ids], A_insert, axis=0)\n",
    "                NA[A_insert_ids] += 1\n",
    "                last_action[A_insert_ids] = 0\n",
    "            if len(X_insert_ids) > 0:\n",
    "                X = np.insert(X, cumsum[X_insert_ids], X_insert, axis=0)\n",
    "                NX[X_insert_ids] += 1\n",
    "                last_action[X_insert_ids] = 1\n",
    "            if len(finished_ids) > 0:\n",
    "                finished[finished_ids] = True\n",
    "            # print finished\n",
    "\n",
    "        return X, A, NX, NA, last_action, finished\n",
    "\n",
    "class Builder(object, metaclass=ABCMeta):\n",
    "\n",
    "    def __init__(self, model_loc, gpu_id=0):\n",
    "        with open(os.path.join(model_loc, 'configs.json')) as f:\n",
    "            configs = json.load(f)\n",
    "\n",
    "        self.mdl = self.__class__._get_model(configs)\n",
    "\n",
    "        self.ctx = mx.gpu(gpu_id) if gpu_id is not None else mx.cpu()\n",
    "        self.mdl.load_parameters(os.path.join(model_loc, 'ckpt.params'), ctx=self.ctx)\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_model(configs):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @abstractmethod\n",
    "    def sample(self, num_samples, *args, **kwargs):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class CVanilla_RNN_Builder(Builder):\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_model(configs):\n",
    "        return CVanillaMolGen_RNN(get_mol_spec().num_atom_types, get_mol_spec().num_bond_types, D=2, **configs)\n",
    "\n",
    "\n",
    "    def sample(self, num_samples, c, output_type='mol', sanitize=True, random=True):\n",
    "        if len(c.shape) == 1:\n",
    "            c = np.stack([c, ]*num_samples, axis=0)\n",
    "\n",
    "        with autograd.predict_mode():\n",
    "            # step one\n",
    "            finished = [False, ] * num_samples\n",
    "\n",
    "            def get_init():\n",
    "                self.mdl.mode = 'decode_0'\n",
    "                _c = nd.array(c, dtype='float32', ctx=self.ctx)\n",
    "                init = self.mdl(_c).asnumpy()\n",
    "                return init\n",
    "\n",
    "            outputs = _decode_step(X=None, A=None, NX=None, NA=None, last_action=None, finished=finished,\n",
    "                                   get_init=get_init, get_action=None,\n",
    "                                   n_node_types=self.mdl.N_A, n_edge_types=self.mdl.N_B,\n",
    "                                   random=random)\n",
    "            X, A, NX, NA, last_action, finished = outputs\n",
    "\n",
    "            count = 1\n",
    "            h = np.zeros([self.mdl.N_rnn, num_samples, self.mdl.F_c[-1]], dtype=np.float32)\n",
    "            while not np.all(finished) and count < 100:\n",
    "                def get_action(inputs):\n",
    "                    self.mdl.mode = 'decode_step'\n",
    "                    _h = nd.array(h[:, np.logical_not(finished), :], ctx=self.ctx, dtype='float32')\n",
    "                    _c = nd.array(c[np.logical_not(finished), :], ctx=self.ctx, dtype='float32')\n",
    "                    _X, _A_sparse, _NX, _NX_rep, _mask, _NX_cum = self.to_nd(inputs)\n",
    "                    _append, _connect, _end, _h = self.mdl(_X, _A_sparse, _NX, _NX_rep, _mask, _NX_cum, _h, _c, _NX_rep)\n",
    "                    h[:, np.logical_not(finished), :] = _h[0].asnumpy()\n",
    "                    return _append.asnumpy(), _connect.asnumpy(), _end.asnumpy()\n",
    "\n",
    "                outputs = _decode_step(X, A, NX, NA, last_action, finished,\n",
    "                                       get_init=None, get_action=get_action,\n",
    "                                       n_node_types=self.mdl.N_A, n_edge_types=self.mdl.N_B,\n",
    "                                       random=random)\n",
    "                X, A, NX, NA, last_action, finished = outputs\n",
    "\n",
    "                count += 1\n",
    "\n",
    "            graph_list = []\n",
    "\n",
    "            cumsum_X_ = np.cumsum(np.pad(NX, [[1, 0]], mode='constant')).tolist()\n",
    "            cumsum_A_ = np.cumsum(np.pad(NA, [[1, 0]], mode='constant')).tolist()\n",
    "\n",
    "            for cumsum_A_pre, cumsum_A_post, \\\n",
    "                cumsum_X_pre, cumsum_X_post in zip(cumsum_A_[:-1], cumsum_A_[1:],\n",
    "                                                   cumsum_X_[:-1], cumsum_X_[1:]):\n",
    "                graph_list.append([X[cumsum_X_pre:cumsum_X_post], A[cumsum_A_pre:cumsum_A_post, :]])\n",
    "\n",
    "            if output_type=='graph':\n",
    "                return graph_list\n",
    "            elif output_type == 'mol':\n",
    "                return get_mol_from_graph_list(graph_list, sanitize)\n",
    "            elif output_type == 'smiles':\n",
    "                mol_list = get_mol_from_graph_list(graph_list, sanitize=True)\n",
    "                smiles_list = [Chem.MolToSmiles(m) if m is not None else None for m in mol_list]\n",
    "                return smiles_list\n",
    "            else:\n",
    "                raise ValueError('Unrecognized output type')\n",
    "\n",
    "    def to_nd(self, inputs):\n",
    "        X, A, NX, NX_rep, mask = inputs[:-2]\n",
    "        NX_cum = np.cumsum(NX)\n",
    "\n",
    "        # convert to ndarray\n",
    "        _to_ndarray = lambda _x: nd.array(_x, self.ctx, 'int64')\n",
    "        X, NX, NX_rep, mask, NX_cum = \\\n",
    "            _to_ndarray(X), _to_ndarray(NX), _to_ndarray(NX_rep), _to_ndarray(mask), _to_ndarray(NX_cum)\n",
    "        A_sparse = []\n",
    "        for _A_i in A:\n",
    "            if _A_i.shape[0] == 0:\n",
    "                A_sparse.append(None)\n",
    "            else:\n",
    "                # transpose may not be supported in gpu\n",
    "                _A_i = np.concatenate([_A_i, _A_i[:, [1, 0]]], axis=0)\n",
    "\n",
    "                # construct csr matrix ...\n",
    "                _data = np.ones((_A_i.shape[0],), dtype=np.float32)\n",
    "                _row, _col = _A_i[:, 0], _A_i[:, 1]\n",
    "                _A_sparse_i = nd.sparse.csr_matrix((_data, (_row, _col)),\n",
    "                                                   shape=tuple([int(X.shape[0]), ] * 2),\n",
    "                                                   ctx=self.ctx, dtype='float32')\n",
    "\n",
    "                # append to list\n",
    "                A_sparse.append(_A_sparse_i)\n",
    "        return X, A_sparse, NX, NX_rep, mask, NX_cum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_samples(protein_data_path, dataset_index, n_samples, run, model_name='CTDGD'):\n",
    "    # Start timer\n",
    "    startTime = int(round(time.time() * 1000))\n",
    "    \n",
    "    # Step 1:\n",
    "    # Get protein embeddings\n",
    "    embeds = []\n",
    "    c = []\n",
    "    \n",
    "    with open(protein_data_path) as f:\n",
    "        for line in f:\n",
    "            embeds.append(line.strip().split('\\t'))\n",
    "    for e in embeds:\n",
    "        c.append([float(e_i) for e_i in e])\n",
    "    c = np.array(c)\n",
    "    \n",
    "    os.system(f'mkdir /workspace/CTDGD/outputs/{model_name}')\n",
    "    os.system(f'mkdir /workspace/CTDGD/outputs/{model_name}/Dataset{dataset_index}')\n",
    "    os.system(f'mkdir /workspace/CTDGD/outputs/{model_name}/Dataset{dataset_index}/model')\n",
    "    os.system(f'mkdir /workspace/CTDGD/outputs/{model_name}/Dataset{dataset_index}/generated_samples')\n",
    "    os.system(f'mkdir /workspace/CTDGD/outputs/{model_name}/Dataset{dataset_index}/generated_samples/{n_samples}')\n",
    "    os.system(f'mkdir /workspace/CTDGD/outputs/{model_name}/Dataset{dataset_index}/generated_samples/{n_samples}/run{run}')\n",
    "    \n",
    "    # Step 2: Loading trained model\n",
    "    model = CVanilla_RNN_Builder(f'/workspace/CTDGD/outputs/{model_name}/Dataset{dataset_index}/model/', gpu_id=0)\n",
    "    \n",
    "    for i in range(len(c)):\n",
    "        # Step 3: Sampling\n",
    "        samples = [m for m in model.sample(n_samples, c=c[i], output_type='graph') if m is not None]\n",
    "        print('Obtained graphs...')\n",
    "        smiles_list = [Chem.MolToSmiles(m) for m in get_mol_from_graph_list(samples, sanitize=True) if m is not None]\n",
    "        print('Obtained SMILES...')\n",
    "\n",
    "        # Step 4: Store smiles in dataframe\n",
    "        smiles_df = pd.DataFrame({'smiles': smiles_list})\n",
    "\n",
    "        # Step 5: Save dataframe in folder\n",
    "        smiles_df.to_csv(f'/workspace/CTDGD/outputs/{model_name}/Dataset{dataset_index}/generated_samples/{n_samples}/run{run}/Protein{i+1}_generated_samples.csv', index=False)\n",
    "\n",
    "    # End timer\n",
    "    endTime = int(round(time.time()))\n",
    "    with open(f'/workspace/CTDGD/outputs/{model_name}/Dataset{dataset_index}/generated_samples/{n_samples}/run{run}/samples_evaluation.txt', 'a+') as eval_f:\n",
    "        eval_f.write(f'Total time: {endTime - startTime} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_index = 2\n",
    "run = 1\n",
    "protein_data_path = f'/workspace/CTDGD/data/d{dataset_index}_te_embeddings_run{run}.txt'\n",
    "n_samples = 1000\n",
    "model_name = 'CTDGD'\n",
    "generate_samples(protein_data_path, dataset_index, n_samples, run, model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>smiles</th>\n",
       "      <th>MW</th>\n",
       "      <th>logP</th>\n",
       "      <th>QED</th>\n",
       "      <th>HD</th>\n",
       "      <th>HA</th>\n",
       "      <th>RB</th>\n",
       "      <th>TPSA</th>\n",
       "      <th>SAS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CC(C)NC(=O)N1CCCC1C(=O)NC(Cc1cccc(C(F)(F)F)c1)...</td>\n",
       "      <td>414.187875</td>\n",
       "      <td>1.80040</td>\n",
       "      <td>0.661132</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>104.53</td>\n",
       "      <td>3.002168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CCC(=O)N(CC1(Cl)CN(c2ccccc2C(N)=O)C1)c1ccccc1</td>\n",
       "      <td>371.140055</td>\n",
       "      <td>3.02630</td>\n",
       "      <td>0.793514</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>66.64</td>\n",
       "      <td>2.614410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CCCC1(C2CCNCC2)OC1c1cccc(OC)c1</td>\n",
       "      <td>275.188529</td>\n",
       "      <td>3.30500</td>\n",
       "      <td>0.837118</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>33.79</td>\n",
       "      <td>3.545915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>O=C(Nc1sc2ccccc2c1CO)c1cccc(C2CN3CCC2CC3)c1</td>\n",
       "      <td>392.155849</td>\n",
       "      <td>4.45510</td>\n",
       "      <td>0.689450</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>52.57</td>\n",
       "      <td>3.685132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CCCC1NC(=O)CCCCCNC(=O)C(Cc2ccccc2)NC(N(C)C)=Nc...</td>\n",
       "      <td>523.298097</td>\n",
       "      <td>4.76972</td>\n",
       "      <td>0.537599</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>85.83</td>\n",
       "      <td>5.266576</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              smiles          MW     logP  \\\n",
       "0  CC(C)NC(=O)N1CCCC1C(=O)NC(Cc1cccc(C(F)(F)F)c1)...  414.187875  1.80040   \n",
       "1      CCC(=O)N(CC1(Cl)CN(c2ccccc2C(N)=O)C1)c1ccccc1  371.140055  3.02630   \n",
       "2                     CCCC1(C2CCNCC2)OC1c1cccc(OC)c1  275.188529  3.30500   \n",
       "3        O=C(Nc1sc2ccccc2c1CO)c1cccc(C2CN3CCC2CC3)c1  392.155849  4.45510   \n",
       "4  CCCC1NC(=O)CCCCCNC(=O)C(Cc2ccccc2)NC(N(C)C)=Nc...  523.298097  4.76972   \n",
       "\n",
       "        QED  HD  HA  RB    TPSA       SAS  \n",
       "0  0.661132   3   3   6  104.53  3.002168  \n",
       "1  0.793514   1   3   6   66.64  2.614410  \n",
       "2  0.837118   1   3   5   33.79  3.545915  \n",
       "3  0.689450   2   4   4   52.57  3.685132  \n",
       "4  0.537599   3   6   4   85.83  5.266576  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_index = 2\n",
    "run = 1\n",
    "n_samples = 1000\n",
    "protein_index = 1\n",
    "model_name = 'CTDGD'\n",
    "smiles_df = pd.read_csv(f'/workspace/CTDGD/outputs/{model_name}/Dataset{dataset_index}/generated_samples/{n_samples}/run{run}/Protein{protein_index}_generated_samples.csv')\n",
    "smiles_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "molecules = [Chem.MolFromSmiles(s) for s in smiles_df['smiles']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random.shuffle(molecules)\n",
    "Draw.MolsToGridImage(molecules[:len(molecules)], molsPerRow=4, maxMols = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
