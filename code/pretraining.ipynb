{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c31ef19-22cc-4e4c-8b94-30d226716d93",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8.18 (default, Sep 11 2023, 13:40:15) \n",
      "[GCC 11.2.0]\n",
      "GPU is available. Number of GPUs: 1\n",
      "Matrix multiplication result:\n",
      "[[247.94977 242.16266 255.54364 ... 252.27475 252.59158 250.49435]\n",
      " [250.44225 244.73228 255.60269 ... 261.77277 252.94102 256.46912]\n",
      " [257.61923 245.85707 257.3741  ... 255.43486 257.95    253.17242]\n",
      " ...\n",
      " [256.43518 252.30537 260.9859  ... 267.22202 260.19785 259.79303]\n",
      " [250.15651 238.36588 253.22952 ... 255.9978  253.5108  256.47482]\n",
      " [244.84286 239.84598 249.69444 ... 250.88753 247.33646 244.66107]]\n",
      "1050937\n",
      "<class 'list'>\n",
      "['O=[N+]([O-])c1cc(C(O)=Nc2cccc(Br)c2)cs1 1', 'O=[N+]([O-])c1ccc(Oc2ccc(Cl)cc2Cl)cc1 1', 'O=[N+]([O-])c1ccc([N+](=O)[O-])c(O)c1 1', 'O=[N+]([O-])c1ccc2cccc3c2c1CC3 1', 'Nc1ccc(C=Cc2ccc([N+](=O)[O-])cc2)cc1 1']\n",
      "656835\n",
      "We are in training part....\n",
      "Training is starting from scratch....\n",
      "We are in policy now\n",
      "shape of X is (15287,) and last_append_mask is (15287,)\n",
      "X is \n",
      "[24 24 45 24 45 45 24 45 45 45]\n",
      "<NDArray 10 @gpu(0)> and last_embed_mask is \n",
      "[1 0 1 0 0 1 0 0 0 1]\n",
      "<NDArray 10 @gpu(0)>\n",
      "shape of X after operation is (15287, 16)\n",
      "We are in policy now\n",
      "shape of X is (14886,) and last_append_mask is (14886,)\n",
      "X is \n",
      "[33 33 24 33 24 45 33 24 45 45]\n",
      "<NDArray 10 @gpu(0)> and last_embed_mask is \n",
      "[1 0 1 0 0 1 0 0 0 1]\n",
      "<NDArray 10 @gpu(0)>\n",
      "shape of X after operation is (14886, 16)\n",
      "We are in policy now\n",
      "shape of X is (15797,) and last_append_mask is (15797,)\n",
      "X is \n",
      "[45 45 24 45 24 45 45 24 45 45]\n",
      "<NDArray 10 @gpu(0)> and last_embed_mask is \n",
      "[1 0 1 0 0 1 0 0 0 1]\n",
      "<NDArray 10 @gpu(0)>\n",
      "shape of X after operation is (15797, 16)\n",
      "We are in policy now\n",
      "shape of X is (15196,) and last_append_mask is (15196,)\n",
      "X is \n",
      "[45 45 45 45 45 45 45 45 45 24]\n",
      "<NDArray 10 @gpu(0)> and last_embed_mask is \n",
      "[1 0 1 0 0 1 0 0 0 1]\n",
      "<NDArray 10 @gpu(0)>\n",
      "shape of X after operation is (15196, 16)\n",
      "We are in policy now\n",
      "shape of X is (14511,) and last_append_mask is (14511,)\n",
      "X is \n",
      "[45 45 45 45 45 45 45 45 45 45]\n",
      "<NDArray 10 @gpu(0)> and last_embed_mask is \n",
      "[1 0 1 0 0 1 0 0 0 1]\n",
      "<NDArray 10 @gpu(0)>\n",
      "shape of X after operation is (14511, 16)\n",
      "We are in policy now\n",
      "shape of X is (14084,) and last_append_mask is (14084,)\n",
      "X is \n",
      "[45 45 45 45 45 45 45 45 45 45]\n",
      "<NDArray 10 @gpu(0)> and last_embed_mask is \n",
      "[1 0 1 0 0 1 0 0 0 1]\n",
      "<NDArray 10 @gpu(0)>\n",
      "shape of X after operation is (14084, 16)\n",
      "We are in policy now\n",
      "shape of X is (13497,) and last_append_mask is (13497,)\n",
      "X is \n",
      "[24 24 24 24 24 45 24 24 45 45]\n",
      "<NDArray 10 @gpu(0)> and last_embed_mask is \n",
      "[1 0 1 0 0 1 0 0 0 1]\n",
      "<NDArray 10 @gpu(0)>\n",
      "shape of X after operation is (13497, 16)\n",
      "We are in policy now\n",
      "shape of X is (14928,) and last_append_mask is (14928,)\n",
      "X is \n",
      "[45 45 45 45 45 24 45 45 24 45]\n",
      "<NDArray 10 @gpu(0)> and last_embed_mask is \n",
      "[1 0 1 0 0 1 0 0 0 1]\n",
      "<NDArray 10 @gpu(0)>\n",
      "shape of X after operation is (14928, 16)\n",
      "We are in policy now\n",
      "shape of X is (15940,) and last_append_mask is (15940,)\n",
      "X is \n",
      "[45 45 45 45 45 45 45 45 45 34]\n",
      "<NDArray 10 @gpu(0)> and last_embed_mask is \n",
      "[1 0 1 0 0 1 0 0 0 1]\n",
      "<NDArray 10 @gpu(0)>\n",
      "shape of X after operation is (15940, 16)\n",
      "We are in policy now\n",
      "shape of X is (15123,) and last_append_mask is (15123,)\n",
      "X is \n",
      "[33 33 45 33 45 45 33 45 45 45]\n",
      "<NDArray 10 @gpu(0)> and last_embed_mask is \n",
      "[1 0 1 0 0 1 0 0 0 1]\n",
      "<NDArray 10 @gpu(0)>\n",
      "shape of X after operation is (15123, 16)\n",
      "We are in policy now\n",
      "shape of X is (15969,) and last_append_mask is (15969,)\n",
      "X is \n",
      "[45 45 45 45 45 45 45 45 45 45]\n",
      "<NDArray 10 @gpu(0)> and last_embed_mask is \n",
      "[1 0 1 0 0 1 0 0 0 1]\n",
      "<NDArray 10 @gpu(0)>\n",
      "shape of X after operation is (15969, 16)\n",
      "We are in policy now\n",
      "shape of X is (13212,) and last_append_mask is (13212,)\n",
      "X is \n",
      "[45 45 45 45 45 45 45 45 45 33]\n",
      "<NDArray 10 @gpu(0)> and last_embed_mask is \n",
      "[1 0 1 0 0 1 0 0 0 1]\n",
      "<NDArray 10 @gpu(0)>\n",
      "shape of X after operation is (13212, 16)\n",
      "We are in policy now\n",
      "shape of X is (16101,) and last_append_mask is (16101,)\n",
      "X is \n",
      "[45 45 45 45 45 33 45 45 33 34]\n",
      "<NDArray 10 @gpu(0)> and last_embed_mask is \n",
      "[1 0 1 0 0 1 0 0 0 1]\n",
      "<NDArray 10 @gpu(0)>\n",
      "shape of X after operation is (16101, 16)\n",
      "We are in policy now\n",
      "shape of X is (15022,) and last_append_mask is (15022,)\n",
      "X is \n",
      "[33 33 45 33 45 45 33 45 45 45]\n",
      "<NDArray 10 @gpu(0)> and last_embed_mask is \n",
      "[1 0 1 0 0 1 0 0 0 1]\n",
      "<NDArray 10 @gpu(0)>\n",
      "shape of X after operation is (15022, 16)\n",
      "We are in policy now\n",
      "shape of X is (15880,) and last_append_mask is (15880,)\n",
      "X is \n",
      "[24 24 24 24 24 45 24 24 45 45]\n",
      "<NDArray 10 @gpu(0)> and last_embed_mask is \n",
      "[1 0 1 0 0 1 0 0 0 1]\n",
      "<NDArray 10 @gpu(0)>\n",
      "shape of X after operation is (15880, 16)\n",
      "We are in policy now\n",
      "shape of X is (14900,) and last_append_mask is (14900,)\n",
      "X is \n",
      "[45 45 45 45 45 45 45 45 45 45]\n",
      "<NDArray 10 @gpu(0)> and last_embed_mask is \n",
      "[1 0 1 0 0 1 0 0 0 1]\n",
      "<NDArray 10 @gpu(0)>\n",
      "shape of X after operation is (14900, 16)\n",
      "We are in policy now\n",
      "shape of X is (15127,) and last_append_mask is (15127,)\n",
      "X is \n",
      "[24 24 45 24 45 33 24 45 33 45]\n",
      "<NDArray 10 @gpu(0)> and last_embed_mask is \n",
      "[1 0 1 0 0 1 0 0 0 1]\n",
      "<NDArray 10 @gpu(0)>\n",
      "shape of X after operation is (15127, 16)\n",
      "We are in policy now\n",
      "shape of X is (15364,) and last_append_mask is (15364,)\n",
      "X is \n",
      "[45 45 45 45 45 24 45 45 24 45]\n",
      "<NDArray 10 @gpu(0)> and last_embed_mask is \n",
      "[1 0 1 0 0 1 0 0 0 1]\n",
      "<NDArray 10 @gpu(0)>\n",
      "shape of X after operation is (15364, 16)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import sys\n",
    "print(sys.version)\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import rdkit\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Draw, QED\n",
    "from rdkit.Chem import rdmolfiles\n",
    "from rdkit.Chem.Draw import IPythonConsole\n",
    "from rdkit.Chem import AllChem\n",
    "\n",
    "from rdkit import RDLogger\n",
    "\n",
    "# Suppress RDKit warnings and errors\n",
    "#RDLogger.DisableLog('rdApp.*')\n",
    "\n",
    "mol = Chem.MolFromSmiles('CO[C@@H]1CC[C@@]2(CC1)Cc1c([C@]32COC(=[NH+]3)N)cc(cc1)c1cncc(c1)C')\n",
    "mol\n",
    "\n",
    "\"\"\"# Importing required packages\"\"\"\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import math\n",
    "import random\n",
    "import argparse\n",
    "import itertools\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from scipy import sparse\n",
    "from mxnet.gluon import nn\n",
    "from collections import Counter\n",
    "from mxnet.autograd import Function\n",
    "from mxnet.gluon.data import Dataset\n",
    "from mxnet import gluon, autograd, nd\n",
    "from mxnet.gluon.data import DataLoader\n",
    "from abc import ABCMeta, abstractmethod\n",
    "from mxnet.gluon.data.sampler import Sampler\n",
    "\n",
    "\"\"\"### Testing the gpu on mxnet\"\"\"\n",
    "\n",
    "import mxnet as mx\n",
    "\n",
    "def check_gpu_availability():\n",
    "    num_gpus = mx.context.num_gpus()\n",
    "    if num_gpus > 0:\n",
    "        print(f\"GPU is available. Number of GPUs: {num_gpus}\")\n",
    "    else:\n",
    "        print(\"No GPU available.\")\n",
    "\n",
    "check_gpu_availability()\n",
    "\n",
    "import mxnet as mx\n",
    "import numpy as np\n",
    "\n",
    "# def matrix_multiplication_on_gpu():\n",
    "#     ctx = mx.gpu() if mx.context.num_gpus() > 0 else mx.cpu()\n",
    "\n",
    "#     # Create random matrices on the GPU\n",
    "#     a = mx.nd.random.uniform(shape=(1000, 1000), ctx=ctx)\n",
    "#     b = mx.nd.random.uniform(shape=(1000, 1000), ctx=ctx)\n",
    "\n",
    "#     # Perform matrix multiplication on the GPU\n",
    "#     result = mx.nd.dot(a, b)\n",
    "\n",
    "#     # Transfer the result back to CPU for printing (optional)\n",
    "#     result_cpu = result.asnumpy()\n",
    "\n",
    "#     print(\"Matrix multiplication result:\")\n",
    "#     print(result_cpu)\n",
    "\n",
    "# matrix_multiplication_on_gpu()\n",
    "\n",
    "\"\"\"# Initializing hyperparameters\"\"\"\n",
    "\n",
    "batch_size = 8   # training batch size\n",
    "batch_size_test = 8   # test batch size\n",
    "k = 5   # number of generation paths\n",
    "p = 0.8   # randomness parameter alpha\n",
    "F_e = 16    # initial hidden embedding size for each node in a graph\n",
    "F_h = [32,64,128,128,256,256]    # output sizes of each GCN layer\n",
    "F_skip = 256    # size of skip connection layer\n",
    "F_c = [512, ]   # hidden sizes of fully connected layers after graph convolution\n",
    "Fh_policy = 128   # hidden size for policy layer\n",
    "activation = 'relu'   # activation function\n",
    "lr = 1e-3   # initial learning rate\n",
    "decay = 1e-3    # initial weight decay\n",
    "decay_step = 100    # perform decay after 100 steps\n",
    "clip_grad = 3.0    # gradient clipping factor\n",
    "summary_step = 500    # store model and training metrics after every certain no. of steps\n",
    "N_rnn = 3   # number of layers used in GRUs\n",
    "is_continuous = False   # load previous model or not\n",
    "model_name = 'base_cdgcn'\n",
    "# model_dir = f'/workspace/Toxicity_experiment/March_experiment/{model_name}'\n",
    "model_dir = f'/workspace/Toxicity_experiment/March_experiment/just_test'\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "# ckpt_dir = f'/workspace/Toxicity_experiment/March_experiment/{model_name}/logs/'    # logs directory\n",
    "ckpt_dir = f'{model_dir}/logs/'\n",
    "os.makedirs(ckpt_dir, exist_ok=True)\n",
    "\"\"\"# Reading data file\"\"\"\n",
    "\n",
    "def read_data(file_name):\n",
    "    smiles = []\n",
    "    with open(file_name) as f:\n",
    "        for line in f:\n",
    "            smiles.append(line.strip())\n",
    "    return smiles\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ChEMBL = '/workspace/Toxicity_experiment/chembl_final.txt'\n",
    "dataset = read_data(ChEMBL)\n",
    "\n",
    "print(len(dataset))\n",
    "print(type(dataset))\n",
    "print(dataset[:5])\n",
    "iterations = (len(dataset)//batch_size)*5    # training iterations\n",
    "single_traverse = len(dataset)//batch_size\n",
    "#print(\"Iterations taken to traverse the whole dataset for one time: \",single_traverse)\n",
    "print(iterations)\n",
    "\n",
    "\"\"\"# Creating mini-batches\"\"\"\n",
    "\n",
    "class BalancedSampler(Sampler):\n",
    "\n",
    "    def __init__(self, cost, batch_size):\n",
    "        index = np.argsort(cost).tolist()\n",
    "        chunk_size = int(float(len(cost))/batch_size)\n",
    "        self.index = []\n",
    "        for i in range(batch_size):\n",
    "            self.index.append(index[i*chunk_size:(i + 1)*chunk_size])\n",
    "\n",
    "    def _g(self):\n",
    "        # shuffle data\n",
    "        for index_i in self.index:\n",
    "            random.shuffle(index_i)\n",
    "\n",
    "        for batch_index in zip(*self.index):\n",
    "            yield batch_index\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self._g()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.index[0])\n",
    "\n",
    "\"\"\"# Obtaining all molecular properties\"\"\"\n",
    "\n",
    "\n",
    "class MoleculeSpec(object):\n",
    "\n",
    "    def __init__(self, file_name='/workspace/binding_data/atom_types.txt'):\n",
    "        self.atom_types = []\n",
    "        self.atom_symbols = []\n",
    "        with open(file_name) as f:\n",
    "            for line in f:\n",
    "                atom_type_i = line.strip('\\n').split(',')\n",
    "                self.atom_types.append((atom_type_i[0], int(atom_type_i[1]), int(atom_type_i[2])))\n",
    "                if atom_type_i[0] not in self.atom_symbols:\n",
    "                    self.atom_symbols.append(atom_type_i[0])\n",
    "        self.bond_orders = [Chem.BondType.AROMATIC,\n",
    "                            Chem.BondType.SINGLE,\n",
    "                            Chem.BondType.DOUBLE,\n",
    "                            Chem.BondType.TRIPLE]\n",
    "        self.max_iter = 120\n",
    "\n",
    "    def get_atom_type(self, atom):\n",
    "        atom_symbol = atom.GetSymbol()\n",
    "        atom_charge = atom.GetFormalCharge()\n",
    "        atom_hs = atom.GetNumExplicitHs()\n",
    "        return self.atom_types.index((atom_symbol, atom_charge, atom_hs))\n",
    "\n",
    "    def get_bond_type(self, bond):\n",
    "        return self.bond_orders.index(bond.GetBondType())\n",
    "\n",
    "    def index_to_atom(self, idx):\n",
    "        atom_symbol, atom_charge, atom_hs = self.atom_types[idx]\n",
    "        a = Chem.Atom(atom_symbol)\n",
    "        a.SetFormalCharge(atom_charge)\n",
    "        a.SetNumExplicitHs(atom_hs)\n",
    "        return a\n",
    "\n",
    "    def index_to_bond(self, mol, begin_id, end_id, idx):\n",
    "        mol.AddBond(begin_id, end_id, self.bond_orders[idx])\n",
    "\n",
    "    @property\n",
    "    def num_atom_types(self):\n",
    "        return len(self.atom_types)\n",
    "\n",
    "    @property\n",
    "    def num_bond_types(self):\n",
    "        return len(self.bond_orders)\n",
    "\n",
    "_mol_spec = None\n",
    "\n",
    "def get_mol_spec():\n",
    "    global _mol_spec\n",
    "    if _mol_spec is None:\n",
    "        _mol_spec = MoleculeSpec()\n",
    "    return _mol_spec\n",
    "\n",
    "\n",
    "\n",
    "# Load ML classifier\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"# Defining utility functions for data preprocessing and postprocessing\"\"\"\n",
    "\n",
    "import mxnet as mx\n",
    "from mxnet import gluon, nd, autograd\n",
    "import numpy as np\n",
    "from rdkit import Chem\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_graph_from_smiles(smiles):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "\n",
    "    # build graph\n",
    "    atom_types, atom_ranks, bonds, bond_types = [], [], [], []\n",
    "    for a, r in zip(mol.GetAtoms(), Chem.CanonicalRankAtoms(mol)):\n",
    "        atom_types.append(get_mol_spec().get_atom_type(a))\n",
    "        atom_ranks.append(r)\n",
    "    for b in mol.GetBonds():\n",
    "        idx_1, idx_2, bt = b.GetBeginAtomIdx(), b.GetEndAtomIdx(), get_mol_spec().get_bond_type(b)\n",
    "        bonds.append([idx_1, idx_2])\n",
    "        bond_types.append(bt)\n",
    "\n",
    "    # build nx graph\n",
    "    graph = nx.Graph()\n",
    "    graph.add_nodes_from(range(len(atom_types)))\n",
    "    graph.add_edges_from(bonds)\n",
    "\n",
    "    return graph, atom_types, atom_ranks, bonds, bond_types\n",
    "\n",
    "\n",
    "def get_graph_from_smiles_list(smiles_list):\n",
    "    graph_list = []\n",
    "    for smiles in smiles_list:\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "\n",
    "        # build graph\n",
    "        atom_types, bonds, bond_types = [], [], []\n",
    "        for a in mol.GetAtoms():\n",
    "            atom_types.append(get_mol_spec().get_atom_type(a))\n",
    "        for b in mol.GetBonds():\n",
    "            idx_1, idx_2, bt = b.GetBeginAtomIdx(), b.GetEndAtomIdx(), get_mol_spec().get_bond_type(b)\n",
    "            bonds.append([idx_1, idx_2])\n",
    "            bond_types.append(bt)\n",
    "\n",
    "        X_0 = np.array(atom_types, dtype=np.int32)\n",
    "        A_0 = np.concatenate([np.array(bonds, dtype=np.int32),\n",
    "                              np.array(bond_types, dtype=np.int32)[:, np.newaxis]],\n",
    "                             axis=1)\n",
    "        graph_list.append([X_0, A_0])\n",
    "    return graph_list\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def traverse_graph(graph, atom_ranks, current_node=None, step_ids=None, p=0.9, log_p=0.0):\n",
    "    if current_node is None:\n",
    "        #print(f\"Traversal has started\")\n",
    "        next_nodes = range(len(atom_ranks))\n",
    "        step_ids = [-1, ] * len(next_nodes)\n",
    "        next_node_ranks = atom_ranks\n",
    "    else:\n",
    "        next_nodes = graph.neighbors(current_node)  # get neighbor nodes\n",
    "        next_nodes = [n for n in next_nodes if step_ids[n] < 0] # filter visited nodes\n",
    "        next_node_ranks = [atom_ranks[n] for n in next_nodes] # get ranks for neighbors\n",
    "    next_nodes = [n for n, r in sorted(zip(next_nodes, next_node_ranks), key=lambda _x:_x[1])] # sort by rank\n",
    "\n",
    "    # iterate through neighbors\n",
    "    while len(next_nodes) > 0:\n",
    "        if len(next_nodes)==1:\n",
    "            next_node = next_nodes[0]\n",
    "        elif random.random() >= (1 - p):\n",
    "            next_node = next_nodes[0]\n",
    "            log_p += np.log(p)\n",
    "        else:\n",
    "            next_node = next_nodes[random.randint(1, len(next_nodes) - 1)]\n",
    "            log_p += np.log((1.0 - p) / (len(next_nodes) - 1))\n",
    "        step_ids[next_node] = max(step_ids) + 1\n",
    "        _, log_p = traverse_graph(graph, atom_ranks, next_node, step_ids, p, log_p)\n",
    "        next_nodes = [n for n in next_nodes if step_ids[n] < 0] # filter visited nodes\n",
    "\n",
    "    \n",
    "    return step_ids, log_p\n",
    "\n",
    "\n",
    "def single_reorder(X_0, A_0, step_ids):\n",
    "    \n",
    "    X_0, A_0 = np.copy(X_0), np.copy(A_0)\n",
    "    \n",
    "\n",
    "    step_ids = np.array(step_ids, dtype=np.int32)\n",
    "\n",
    "    # sort by step_ids\n",
    "    sorted_ids = np.argsort(step_ids)\n",
    "    X_0 = X_0[sorted_ids]\n",
    "    A_0[:, 0], A_0[:, 1] = step_ids[A_0[:, 0]], step_ids[A_0[:, 1]]\n",
    "    # max_b is the max_index of each bond,min_b is the min index of each index\n",
    "    max_b, min_b = np.amax(A_0[:, :2], axis=1), np.amin(A_0[:, :2], axis=1)\n",
    "    A_0 = A_0[np.lexsort([-min_b, max_b]), :]\n",
    "\n",
    "    # separate append and connect\n",
    "    max_b, min_b = np.amax(A_0[:, :2], axis=1), np.amin(A_0[:, :2], axis=1)\n",
    "    is_append = np.concatenate([np.array([True]), max_b[1:] > max_b[:-1]])\n",
    "    A_0 = np.concatenate([np.where(is_append[:, np.newaxis],\n",
    "                                 np.stack([min_b, max_b], axis=1),\n",
    "                                 np.stack([max_b, min_b], axis=1)),\n",
    "                        A_0[:, -1:]], axis=1)\n",
    "    \n",
    "    return X_0, A_0\n",
    "\n",
    "\n",
    "def single_expand(X_0, A_0):\n",
    "    #print(f\"We are in single expand function\")\n",
    "    X_0, A_0 = np.copy(X_0), np.copy(A_0)\n",
    "\n",
    "    # expand X\n",
    "\n",
    "    is_append_iter = np.less(A_0[:, 0], A_0[:, 1]).astype(np.int32)\n",
    "    \n",
    "    \n",
    "    NX = np.cumsum(np.pad(is_append_iter, [[1, 0]], mode='constant', constant_values=1))\n",
    "    shift = np.cumsum(np.pad(NX, [[1, 0]], mode='constant')[:-1])\n",
    "    X_index = np.arange(NX.sum(), dtype=np.int32) - np.repeat(shift, NX)\n",
    "\n",
    "    \n",
    "    X = X_0[X_index]\n",
    "   \n",
    "    _, A_index = np.tril_indices(A_0.shape[0])\n",
    "   \n",
    "    A = A_0[A_index, :]\n",
    "   \n",
    "    NA = np.arange(A_0.shape[0] + 1)                  # Num_of_bonds + 1\n",
    "    \n",
    "    action_type = 1 - is_append_iter\n",
    "    atom_type = np.where(action_type == 0, X_0[A_0[:, 1]], 0)\n",
    "    bond_type = A_0[:, 2]\n",
    "    append_pos = np.where(action_type == 0, A_0[:, 0], 0)\n",
    "    connect_pos = np.where(action_type == 1, A_0[:, 1], 0)\n",
    "    actions = np.stack([action_type, atom_type, bond_type, append_pos, connect_pos],\n",
    "                       axis=1)\n",
    "    last_action = [[2, 0, 0, 0, 0]]\n",
    "    actions = np.append(actions, last_action, axis=0)\n",
    "    #print(f\"actions is {actions}\")\n",
    "    action_0 = np.array([X_0[0]], dtype=np.int32)\n",
    "    #print(f\"action_0 is {action_0}\")\n",
    "\n",
    "    \n",
    "    last_atom_index = shift + NX - 1\n",
    "    last_atom_mask = np.zeros_like(X)\n",
    "    last_atom_mask[last_atom_index] = np.where(\n",
    "        np.pad(is_append_iter, [[1, 0]], mode='constant', constant_values=1) == 1,\n",
    "        np.ones_like(last_atom_index),\n",
    "        np.ones_like(last_atom_index) * 2)\n",
    "    \n",
    "\n",
    "    return action_0, X, NX, A, NA, actions, last_atom_mask\n",
    "\n",
    "\n",
    "def get_d(A, X):\n",
    "    \n",
    "    _to_sparse = lambda _A, _X: sparse.coo_matrix((np.ones([_A.shape[0] * 2], dtype=np.int32),\n",
    "                                                   (np.concatenate([_A[:, 0], _A[:, 1]], axis=0),\n",
    "                                                    np.concatenate([_A[:, 1], _A[:, 0]], axis=0))),\n",
    "                                                    shape=[_X.shape[0], ] * 2)   # this is creating a square matrtix of size len(X)\n",
    "\n",
    "\n",
    "    A_sparse = _to_sparse(A, X)\n",
    "\n",
    "    d2 = A_sparse * A_sparse\n",
    "    d3 = d2 * A_sparse\n",
    "\n",
    "    D_2 = np.stack(d2.nonzero(), axis=1)\n",
    "    D_2 = D_2[D_2[:, 0] < D_2[:, 1], :]\n",
    "\n",
    "    D_3 = np.stack(d3.nonzero(), axis=1)\n",
    "    D_3 = D_3[D_3[:, 0] < D_3[:, 1], :]\n",
    "\n",
    "    D_3_sparse = _to_sparse(D_3, X)\n",
    "    D_3_sparse = D_3_sparse - D_3_sparse.multiply(A_sparse)\n",
    "    D_3 = np.stack(D_3_sparse.nonzero(), axis=1)\n",
    "    D_3 = D_3[D_3[:, 0] < D_3[:, 1], :]\n",
    "\n",
    "    return D_2, D_3\n",
    "\n",
    "\n",
    "def merge_single_0(X_0, A_0, NX_0, NA_0):\n",
    "\n",
    "    cumsum = np.cumsum(np.pad(NX_0, [[1, 0]], mode='constant')[:-1])\n",
    "\n",
    "    A_0[:, :2] += np.stack([np.repeat(cumsum, NA_0), ] * 2, axis=1)\n",
    "\n",
    "    D_0_2, D_0_3 = get_d(A_0, X_0)\n",
    "\n",
    "    A_split = []\n",
    "    for i in range(get_mol_spec().num_bond_types):\n",
    "        A_i = A_0[A_0[:, 2] == i, :2]\n",
    "        A_split.append(A_i)\n",
    "    A_split.extend([D_0_2, D_0_3])\n",
    "    A_0 = A_split\n",
    "\n",
    "\n",
    "    NX_rep_0 = np.repeat(np.arange(NX_0.shape[0]), NX_0)\n",
    "\n",
    "\n",
    "    return X_0, A_0, NX_0, NX_rep_0\n",
    "\n",
    "\n",
    "def merge_single(X, A, NX, NA, mol_ids, rep_ids, iw_ids, action_0, actions, last_append_mask, log_p):\n",
    "    \n",
    "    X, A, NX, NX_rep = merge_single_0(X, A, NX, NA)\n",
    "    cumsum = np.cumsum(np.pad(NX, [[1, 0]], mode='constant')[:-1])\n",
    "    actions[:, -2] += cumsum * (actions[:, 0] == 0)\n",
    "    actions[:, -1] += cumsum * (actions[:, 0] == 1)\n",
    "    mol_ids_rep = np.repeat(mol_ids, NX)\n",
    "    rep_ids_rep = np.repeat(rep_ids, NX)\n",
    "\n",
    "    return X, A,\\\n",
    "           mol_ids_rep, rep_ids_rep, iw_ids,\\\n",
    "           last_append_mask,\\\n",
    "           NX, NX_rep,\\\n",
    "           action_0, actions, \\\n",
    "           log_p\n",
    "\n",
    "\n",
    "\n",
    "def process_single(smiles, k, p):\n",
    "\n",
    "    smiles, smiles_class = smiles.split(\" \")\n",
    "    smiles_class = int(smiles_class)\n",
    "    graph, atom_types, atom_ranks, bonds, bond_types = get_graph_from_smiles(smiles)\n",
    "\n",
    "    try:\n",
    "        graph, atom_types, atom_ranks, bonds, bond_types = get_graph_from_smiles(smiles)\n",
    "        X_0 = np.array(atom_types, dtype=np.int32)\n",
    "        A_0 = np.concatenate([np.array(bonds, dtype=np.int32),\n",
    "                                  np.array(bond_types, dtype=np.int32)[:, np.newaxis]], axis=1)\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "            print(f\"Error processing SMILES: {smiles}\")\n",
    "            print(f\"Error message: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\"m = Chem.MolFromSmiles(smiles)\n",
    "    qed_smiles = QED.qed(m)\n",
    "    if qed_smiles > 0.50:\n",
    "        prop_class = 0\n",
    "    else :\n",
    "        prop_class = 1\"\"\"   \n",
    "    \n",
    "    \n",
    "    X, A = [], []\n",
    "    NX, NA = [], []\n",
    "    mol_ids, rep_ids, iw_ids = [], [], []\n",
    "    action_0, actions = [], []\n",
    "    last_append_mask = []\n",
    "    log_p = []\n",
    "    tox_class = [1] * k\n",
    "\n",
    "    # random sampling decoding route\n",
    "    #print(f\"We are starting sampling traversals for this smiles {smiles}\")\n",
    "\n",
    "    for i in range(k):\n",
    "        \n",
    "\n",
    "        step_ids_i, log_p_i = traverse_graph(graph, atom_ranks, p=p)\n",
    "        \n",
    "        X_i, A_i = single_reorder(X_0, A_0, step_ids_i)\n",
    "        action_0_i, X_i, NX_i, A_i, NA_i, actions_i, last_atom_mask_i = single_expand(X_i, A_i)\n",
    "\n",
    "        # appends\n",
    "        X.append(X_i)\n",
    "        A.append(A_i)\n",
    "        NX.append(NX_i)\n",
    "        NA.append(NA_i)\n",
    "        action_0.append(action_0_i)\n",
    "        actions.append(actions_i)\n",
    "        last_append_mask.append(last_atom_mask_i)\n",
    "\n",
    "        mol_ids.append(np.zeros_like(NX_i, dtype=np.int32))\n",
    "        rep_ids.append(np.ones_like(NX_i, dtype=np.int32) * i)\n",
    "        iw_ids.append(np.ones_like(NX_i, dtype=np.int32) * i)\n",
    "\n",
    "        log_p.append(log_p_i)\n",
    "        #tox_class.append(tox_class_i)\n",
    "\n",
    "    # concatenate\n",
    "    X = np.concatenate(X, axis=0)\n",
    "    A = np.concatenate(A, axis = 0)\n",
    "    NX = np.concatenate(NX, axis = 0)\n",
    "    NA = np.concatenate(NA, axis = 0)\n",
    "    action_0 = np.concatenate(action_0, axis = 0)\n",
    "    actions = np.concatenate(actions, axis = 0)\n",
    "    last_append_mask = np.concatenate(last_append_mask, axis = 0)\n",
    "    mol_ids = np.concatenate(mol_ids, axis = 0)\n",
    "    rep_ids = np.concatenate(rep_ids, axis = 0)\n",
    "    iw_ids = np.concatenate(iw_ids, axis = 0)\n",
    "    log_p = np.array(log_p, dtype=np.float32)\n",
    "    tox_class = np.array(tox_class)\n",
    "\n",
    "    return X, A, NX, NA, mol_ids, rep_ids, iw_ids, action_0, actions, last_append_mask, log_p, tox_class\n",
    "\n",
    "\n",
    "# noinspection PyArgumentList\n",
    "def get_mol_from_graph(X, A, sanitize=True):\n",
    "    try:\n",
    "        mol = Chem.RWMol(Chem.Mol())\n",
    "\n",
    "        X, A = X.tolist(), A.tolist()\n",
    "        for i, atom_type in enumerate(X):\n",
    "            mol.AddAtom(get_mol_spec().index_to_atom(atom_type))\n",
    "\n",
    "        for atom_id1, atom_id2, bond_type in A:\n",
    "            get_mol_spec().index_to_bond(mol, atom_id1, atom_id2, bond_type)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "    if sanitize:\n",
    "        try:\n",
    "            mol = mol.GetMol()\n",
    "            Chem.SanitizeMol(mol)\n",
    "            return mol\n",
    "        except:\n",
    "            return None\n",
    "    else:\n",
    "        return mol\n",
    "\n",
    "def get_mol_from_graph_list(graph_list, sanitize=True):\n",
    "    mol_list = [get_mol_from_graph(X, A, sanitize) for X, A in graph_list]\n",
    "    return mol_list\n",
    "\n",
    "\n",
    "# Defining dataloader\n",
    "\n",
    "class MolLoader(DataLoader):\n",
    "    \"\"\"Load graph based molecule representation from SMILES\"\"\"\n",
    "    def __init__(self, dataset, batch_size=10, num_workers=0,\n",
    "                 k=10, p=0.9, shuffle=False, sampler=None, batch_sampler=None):\n",
    "        self.k = k\n",
    "        self.p = p\n",
    "\n",
    "        # batch_sampler, sampler and shuffle are mutually exclusive\n",
    "        if batch_sampler is not None:\n",
    "            super(MolLoader, self).__init__(dataset, batch_sampler=batch_sampler,\n",
    "                                            num_workers=num_workers, batchify_fn=self._collate_fn)\n",
    "        elif sampler is not None:\n",
    "            super(MolLoader, self).__init__(dataset, sampler=sampler,\n",
    "                                            num_workers=num_workers, batchify_fn=self._collate_fn,\n",
    "                                            last_batch='rollover')\n",
    "        else:\n",
    "            super(MolLoader, self).__init__(dataset, batch_size, shuffle=shuffle,\n",
    "                                            num_workers=num_workers, batchify_fn=self._collate_fn,\n",
    "                                            last_batch='rollover')\n",
    "\n",
    "\n",
    "    def _collate_fn(self, batch):\n",
    "        # names = X, A,\n",
    "        #         NX, NA,\n",
    "        #         mol_ids, rep_ids, iw_ids,\n",
    "        #         action_0, actions,\n",
    "        #         last_append_mask,\n",
    "        #         log_p\n",
    "\n",
    "        shapes = [[0], [0, 3],\n",
    "                  [0], [0],\n",
    "                  [0], [0], [0],\n",
    "                  [0], [0, 5],\n",
    "                  [0],\n",
    "                  [0], [0]]\n",
    "        dtypes = [np.int32, np.int32,\n",
    "                  np.int32, np.int32,\n",
    "                  np.int32, np.int32, np.int32,\n",
    "                  np.int32, np.int32,\n",
    "                  np.int32,\n",
    "                  np.float32, np.int32]\n",
    "\n",
    "        _build = lambda: [np.zeros(shape=s, dtype=d) for s, d in zip(shapes, dtypes)]\n",
    "        _append = lambda _r0, _r1: [np.concatenate([__r0, __r1], axis=0)\n",
    "                                    for __r0, __r1 in zip(_r0, _r1)]\n",
    "\n",
    "        X, A, \\\n",
    "        NX, NA, \\\n",
    "        mol_ids, rep_ids, iw_ids, \\\n",
    "        action_0, actions, \\\n",
    "        last_append_mask, \\\n",
    "        log_p, tox_class = _build()\n",
    "\n",
    "        # Processing batch of molecules\n",
    "        #print(f\"This is new batch\")\n",
    "        for i, record_in in enumerate(batch):\n",
    "            #print(f\"{i}th smiles is {record_in}\")\n",
    "            smiles = record_in\n",
    "\n",
    "            # Using process single\n",
    "            X_i, A_i, \\\n",
    "            NX_i, NA_i, \\\n",
    "            mol_ids_i, rep_ids_i, iw_ids_i, \\\n",
    "            action_0_i, actions_i, \\\n",
    "            last_append_mask_i, log_p_i, tox_class_i = process_single(smiles, self.k, self.p)\n",
    "\n",
    "            if i != 0:\n",
    "                mol_ids_i += mol_ids[-1] + 1\n",
    "                iw_ids_i += iw_ids[-1] + 1\n",
    "\n",
    "            # appending to the list\n",
    "            tox_class_i = tox_class_i.reshape(-1)\n",
    "\n",
    "\n",
    "            X, A, \\\n",
    "            NX, NA, \\\n",
    "            mol_ids, rep_ids, iw_ids, \\\n",
    "            action_0, actions, \\\n",
    "            last_append_mask, \\\n",
    "            log_p, tox_class = _append([X, A,\n",
    "                             NX, NA,\n",
    "                             mol_ids, rep_ids, iw_ids,\n",
    "                             action_0, actions,\n",
    "                             last_append_mask,\n",
    "                             log_p, tox_class],\n",
    "                            [X_i, A_i,\n",
    "                             NX_i, NA_i,\n",
    "                             mol_ids_i, rep_ids_i, iw_ids_i,\n",
    "                             action_0_i, actions_i,\n",
    "                             last_append_mask_i,\n",
    "                             log_p_i, tox_class_i])\n",
    "\n",
    "        \n",
    "        # using merge single\n",
    "        X, A, \\\n",
    "        mol_ids_rep, rep_ids_rep, iw_ids, \\\n",
    "        last_append_mask, \\\n",
    "        NX, NX_rep, \\\n",
    "        action_0, actions, \\\n",
    "        log_p = merge_single(X, A,\n",
    "                                   NX, NA,\n",
    "                                   mol_ids, rep_ids, iw_ids,\n",
    "                                   action_0, actions,\n",
    "                                   last_append_mask,\n",
    "                                   log_p)\n",
    "        \n",
    "\n",
    "        # returning all the data structures combined as list\n",
    "        result_out = [X, A,\n",
    "                      mol_ids_rep, rep_ids_rep, iw_ids,\n",
    "                      last_append_mask,\n",
    "                      NX, NX_rep,\n",
    "                      action_0, actions,\n",
    "                      log_p, tox_class]\n",
    "\n",
    "        return result_out\n",
    "\n",
    "    @staticmethod\n",
    "    def from_numpy_to_tensor(record):\n",
    "        \"\"\"Convert numpy to tensor and place it to a specific device\"\"\"\n",
    "        [X, A,\n",
    "         mol_ids_rep, rep_ids_rep, iw_ids,\n",
    "         last_append_mask,\n",
    "         NX, NX_rep,\n",
    "         action_0, actions,\n",
    "         log_p, tox_class] = record\n",
    "\n",
    "        \n",
    "        X = nd.array(X, ctx=mx.gpu(), dtype='int32')\n",
    "\n",
    "        # They are tryign\n",
    "        A_sparse = []\n",
    "        #print(f\"This is a batch for numpy_to_tensor\")\n",
    "        for A_i in A:\n",
    "            \n",
    "            if A_i.shape[0] == 0:\n",
    "                A_sparse.append(None)\n",
    "                #print(f\"We had to append none to A_sparse\")\n",
    "            else:\n",
    "                # transpose may not be supported in gpu\n",
    "                A_i = np.concatenate([A_i, A_i[:, [1, 0]]], axis=0)\n",
    "\n",
    "                # construct csr matrix ...\n",
    "                data = np.ones((A_i.shape[0], ), dtype=np.float32)\n",
    "                row, col = A_i[:, 0], A_i[:, 1]\n",
    "                A_sparse_i = nd.sparse.csr_matrix((data, (row, col)),\n",
    "                                                  shape=tuple([int(X.shape[0]), ]*2),\n",
    "                                                  ctx=mx.gpu(),\n",
    "                                                  dtype='float32')\n",
    "\n",
    "                \n",
    "                A_sparse.append(A_sparse_i)\n",
    "\n",
    "        batch_size, iw_size = (mol_ids_rep.max() + 1).item(), \\\n",
    "                              (rep_ids_rep.max() + 1).item()\n",
    "\n",
    "        mol_ids_rep, rep_ids_rep, iw_ids, \\\n",
    "        last_append_mask, \\\n",
    "        NX, NX_rep, action_0, actions = [nd.array(_x, ctx=mx.gpu(), dtype='int32')\n",
    "                                         for _x in [mol_ids_rep, rep_ids_rep, iw_ids,\n",
    "                                                    last_append_mask,\n",
    "                                                    NX, NX_rep, action_0, actions]]\n",
    "\n",
    "        log_p = nd.array(log_p, ctx=mx.gpu(), dtype='float32')\n",
    "        tox_class = nd.array(tox_class, ctx=mx.gpu())\n",
    "        #print(f\"Length of log_p is {log_p}\")\n",
    "        record = [X, A_sparse, iw_ids, last_append_mask,\n",
    "                  NX, NX_rep, action_0, actions, log_p,\n",
    "                  batch_size, iw_size, tox_class]\n",
    "\n",
    "\n",
    "        return record\n",
    "\n",
    "\n",
    "class MolRNNLoader(MolLoader):\n",
    "\n",
    "    def _collate_fn(self, batch):\n",
    "        result_out = super(MolRNNLoader, self)._collate_fn(batch)\n",
    "\n",
    "        # things ready for rnn\n",
    "        batch = [smile.split(\" \")[0] for smile in batch]\n",
    "        \n",
    "        mol_list = [Chem.MolFromSmiles(batch_i) for batch_i in batch]\n",
    "        \n",
    "        graph_to_rnn = np.zeros((len(batch), self.k, get_mol_spec().max_iter), dtype=np.int32)\n",
    "        rnn_to_graph = []\n",
    "        cum_sum = 0\n",
    "        for i, mol_i in enumerate(mol_list):\n",
    "            num_iter = mol_i.GetNumBonds() + 1\n",
    "            for k in range(self.k):\n",
    "                graph_to_rnn[i, k, :num_iter] = (np.arange(num_iter) + cum_sum)\n",
    "\n",
    "                rnn_to_graph_0 = np.ones([num_iter,], dtype=np.int32) * i\n",
    "                rnn_to_graph_1 = np.ones_like(rnn_to_graph_0) * k\n",
    "                rnn_to_graph_2 = np.arange(num_iter)\n",
    "                rnn_to_graph.append(np.stack([rnn_to_graph_0, rnn_to_graph_1, rnn_to_graph_2], axis=0))\n",
    "\n",
    "                cum_sum += num_iter\n",
    "        rnn_to_graph = np.concatenate(rnn_to_graph, axis=1)\n",
    "        NX_cum = np.cumsum(result_out[6])\n",
    "\n",
    "        result_out = result_out + [graph_to_rnn, rnn_to_graph, NX_cum]\n",
    "\n",
    "        return result_out\n",
    "\n",
    "    @staticmethod\n",
    "    def from_numpy_to_tensor(record):\n",
    "        [X, A,\n",
    "         mol_ids_rep, rep_ids_rep, iw_ids,\n",
    "         last_append_mask,\n",
    "         NX, NX_rep,\n",
    "         action_0, actions,\n",
    "         log_p, tox_class,\n",
    "         graph_to_rnn, rnn_to_graph, NX_cum] = record\n",
    "\n",
    "        output = MolLoader.from_numpy_to_tensor([X, A,\n",
    "                                                 mol_ids_rep, rep_ids_rep, iw_ids,\n",
    "                                                 last_append_mask,\n",
    "                                                 NX, NX_rep,\n",
    "                                                 action_0, actions,\n",
    "                                                 log_p, tox_class])\n",
    "\n",
    "        graph_to_rnn, rnn_to_graph, NX_cum =\\\n",
    "            nd.array(graph_to_rnn, ctx=mx.gpu(), dtype='int32'),\\\n",
    "            nd.array(rnn_to_graph, ctx=mx.gpu(), dtype='int32'), \\\n",
    "            nd.array(NX_cum, ctx=mx.gpu(), dtype='int32')\n",
    "\n",
    "        output = output + [graph_to_rnn, rnn_to_graph, NX_cum]\n",
    "\n",
    "        return output\n",
    "\n",
    "\"\"\"# Defining graph convolution and other functions\"\"\"\n",
    "\n",
    "class GraphConvFn(Function):\n",
    "\n",
    "    def __init__(self, A):\n",
    "\n",
    "        self.A = A # type: nd.sparse.CSRNDArray\n",
    "        self.A_T = self.A # assume symmetric\n",
    "        super(GraphConvFn, self).__init__()\n",
    "\n",
    "    def forward(self, X):\n",
    "        \n",
    "\n",
    "        if self.A is not None:\n",
    "            #print(f\"Shape of A is {self.A.shape}\")\n",
    "\n",
    "            if len(X.shape) > 2:\n",
    "                X_resized = X.reshape((X.shape[0], -1))\n",
    "                output = nd.sparse.dot(self.A, X_resized)      # dot product of adjacency and features matrix\n",
    "                output = output.reshape([-1, ] + [X.shape[i] for i in range(1, len(X.shape))])\n",
    "            else:\n",
    "                output = nd.sparse.dot(self.A, X)\n",
    "            return output\n",
    "        else:\n",
    "            return nd.zeros_like(X)\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "\n",
    "        if self.A is not None:\n",
    "            if len(grad_output.shape) > 2:\n",
    "                grad_output_resized = grad_output.reshape((grad_output.shape[0], -1))\n",
    "                grad_input = nd.sparse.dot(self.A_T, grad_output_resized)\n",
    "                grad_input = grad_input.reshape([-1] + [grad_output.shape[i]\n",
    "                                                        for i in range(1, len(grad_output.shape))])\n",
    "            else:\n",
    "                grad_input = nd.sparse.dot(self.A_T, grad_output)\n",
    "            return grad_input\n",
    "        else:\n",
    "            return nd.zeros_like(grad_output)\n",
    "\n",
    "\n",
    "class EfficientGraphConvFn(Function):\n",
    "    \"\"\"Save memory by re-computation\"\"\"\n",
    "\n",
    "    def __init__(self, A_list):\n",
    "        self.A_list = A_list\n",
    "        super(EfficientGraphConvFn, self).__init__()\n",
    "\n",
    "    def forward(self, X, W):\n",
    "       # print(f\"We are in efficient_graph_convolution\")\n",
    "       # print(f\"length of A_list is {len(self.A_list)}\")\n",
    "        X_list = [X]\n",
    "        for A in self.A_list:\n",
    "            if A is not None:\n",
    "                X_list.append(nd.sparse.dot(A, X))\n",
    "            else:\n",
    "                X_list.append(nd.zeros_like(X))\n",
    "        X_out = nd.concat(*X_list, dim=1)\n",
    "        self.save_for_backward(X, W)\n",
    "        # print(f\"X_out shape is{X_out.shape} and W shape is {W.shape}\")\n",
    "        return nd.dot(X_out, W)\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        X, W = self.saved_tensors\n",
    "\n",
    "        # recompute X_out\n",
    "        X_list = [X, ]\n",
    "        for A in self.A_list:\n",
    "            if A is not None:\n",
    "                X_list.append(nd.sparse.dot(A, X))\n",
    "            else:\n",
    "                X_list.append(nd.zeros_like(X))\n",
    "        X_out = nd.concat(*X_list, dim=1)\n",
    "\n",
    "        grad_W = nd.dot(X_out.T, grad_output)\n",
    "\n",
    "        grad_X_out = nd.dot(grad_output, W.T)\n",
    "        grad_X_out_list = nd.split(grad_X_out, num_outputs=len(self.A_list) + 1)\n",
    "\n",
    "\n",
    "        grad_X = [grad_X_out_list[0], ]\n",
    "        for A, grad_X_out in zip(self.A_list, grad_X_out_list[1:]):\n",
    "            if A is not None:\n",
    "                grad_X.append(nd.sparse.dot(A, grad_X_out))\n",
    "            else:\n",
    "                grad_X.append(nd.zeros_like(grad_X_out))\n",
    "\n",
    "        grad_X = sum(grad_X)\n",
    "\n",
    "        return grad_X, grad_W\n",
    "\n",
    "\n",
    "class SegmentSumFn(GraphConvFn):\n",
    "\n",
    "    def __init__(self, idx, num_seg):\n",
    "        # build A\n",
    "        # construct coo\n",
    "        data = nd.ones(idx.shape[0], ctx=idx.context, dtype='int32')\n",
    "        row, col = idx, nd.arange(idx.shape[0], ctx=idx.context, dtype='int32')\n",
    "        shape = (num_seg, int(idx.shape[0]))\n",
    "        sparse = nd.sparse.csr_matrix((data, (row, col)), shape=shape,\n",
    "                                      ctx=idx.context, dtype='float32')\n",
    "        super(SegmentSumFn, self).__init__(sparse)\n",
    "\n",
    "        sparse = nd.sparse.csr_matrix((data, (col, row)), shape=(shape[1], shape[0]),\n",
    "                                      ctx=idx.context, dtype='float32')\n",
    "        self.A_T = sparse\n",
    "\n",
    "\n",
    "def squeeze(input, axis):\n",
    "    assert input.shape[axis] == 1\n",
    "\n",
    "    new_shape = list(input.shape)\n",
    "    del new_shape[axis]\n",
    "\n",
    "    return input.reshape(new_shape)\n",
    "\n",
    "\n",
    "def unsqueeze(input, axis):\n",
    "    return nd.expand_dims(input, axis=axis)\n",
    "\n",
    "\n",
    "def logsumexp(inputs, axis=None, keepdims=False):\n",
    "    \"\"\"Numerically stable logsumexp.\n",
    "    Args:\n",
    "        inputs: A Variable with any shape.\n",
    "        axis: An integer.\n",
    "        keepdims: A boolean.\n",
    "    Returns:\n",
    "        Equivalent of log(sum(exp(inputs), dim=dim, keepdim=keepdim)).\n",
    "    Adopted from: https://github.com/pytorch/pytorch/issues/2591\n",
    "    \"\"\"\n",
    "    # For a 1-D array x (any array along a single dimension),\n",
    "    # log sum exp(x) = s + log sum exp(x - s)\n",
    "    # with s = max(x) being a common choice.\n",
    "    if axis is None:\n",
    "        inputs = inputs.reshape([-1])\n",
    "        axis = 0\n",
    "    s = nd.max(inputs, axis=axis, keepdims=True)\n",
    "    outputs = s + (inputs - s).exp().sum(axis=axis, keepdims=True).log()\n",
    "    if not keepdims:\n",
    "        outputs = nd.sum(outputs, axis=axis, keepdims=False)\n",
    "    return outputs\n",
    "\n",
    "\n",
    "def get_activation(name):\n",
    "    activation_dict = {\n",
    "        'relu':nd.relu,\n",
    "        'tanh':nd.tanh\n",
    "    }\n",
    "    return activation_dict[name]\n",
    "\n",
    "\"\"\"# Defining neural networks\"\"\"\n",
    "\n",
    "class Linear_BN(nn.Sequential):\n",
    "    def __init__(self, F_in, F_out):\n",
    "        super(Linear_BN, self).__init__()\n",
    "       # print(f\"Dense layer of input {F_in} and output {F_out} is created \")\n",
    "        self.add(nn.Dense(F_out, in_units=F_in, use_bias=False))\n",
    "        self.add(BatchNorm(in_channels=F_out))\n",
    "\n",
    "\n",
    "class GraphConv(nn.Block):\n",
    "\n",
    "    def __init__(self, Fin, Fout, D):\n",
    "        super(GraphConv, self).__init__()\n",
    "\n",
    "        # model settings\n",
    "        self.Fin = Fin\n",
    "        self.Fout = Fout\n",
    "        self.D = D\n",
    "       \n",
    "        # model parameters\n",
    "        self.W = self.params.get('w', shape=(self.Fin * (self.D + 1), self.Fout),\n",
    "                                 init=None, allow_deferred_init=False)\n",
    "\n",
    "    def forward(self, X, A_list):\n",
    "        try:\n",
    "            assert len(A_list) == self.D\n",
    "        except AssertionError as e:\n",
    "            print(self.D, len(A_list))\n",
    "            raise e\n",
    "        return EfficientGraphConvFn(A_list)(X, self.W.data(X.context))\n",
    "\n",
    "\n",
    "class Policy(nn.Block):\n",
    "\n",
    "    def __init__(self, F_in, F_h, N_A, N_B, k=1):\n",
    "        super(Policy, self).__init__()\n",
    "        self.F_in = F_in # number of input features for each atom\n",
    "        self.F_h = F_h # number of context variables\n",
    "        self.N_A = N_A # number of atom types\n",
    "        self.N_B = N_B # number of bond types\n",
    "        self.k = k # number of softmax used in the mixture\n",
    "\n",
    "\n",
    "        with self.name_scope():\n",
    "            self.linear_h = Linear_BN(F_in * 2, self.F_h * k)\n",
    "            self.linear_h_t = Linear_BN(F_in, self.F_h * k)\n",
    "\n",
    "            self.linear_x = nn.Dense(self.N_B + self.N_B*self.N_A, in_units=self.F_h)\n",
    "            self.linear_x_t = nn.Dense(1, in_units=self.F_h)\n",
    "\n",
    "            if self.k > 1:\n",
    "                self.linear_pi = nn.Dense(self.k, in_units=self.F_in)\n",
    "            else:\n",
    "                self.linear_pi = None\n",
    "\n",
    "    def forward(self, X, NX, NX_rep, X_end=None):\n",
    "        # segment mean for X\n",
    "        if X_end is None:\n",
    "            X_end = SegmentSumFn(NX_rep, NX.shape[0])(X)/nd.cast(unsqueeze(NX, 1), 'float32')\n",
    "        X = nd.concat(X, X_end[NX_rep, :], dim=1)\n",
    "\n",
    "        X_h = nd.relu(self.linear_h(X)).reshape([-1, self.F_h])\n",
    "        X_h_end = nd.relu(self.linear_h_t(X_end)).reshape([-1, self.F_h])\n",
    "\n",
    "        X_x = nd.exp(self.linear_x(X_h)).reshape([-1, self.k, self.N_B + self.N_B*self.N_A])\n",
    "        X_x_end = nd.exp(self.linear_x_t(X_h_end)).reshape([-1, self.k, 1])\n",
    "\n",
    "        X_sum = nd.sum(SegmentSumFn(NX_rep, NX.shape[0])(X_x), -1, keepdims=True) + X_x_end\n",
    "        X_sum_gathered = X_sum[NX_rep, :, :]\n",
    "\n",
    "        X_softmax = X_x / X_sum_gathered\n",
    "        X_softmax_end = X_x_end/ X_sum\n",
    "\n",
    "        if self.k > 1:\n",
    "            pi = unsqueeze(nd.softmax(self.linear_pi(X_end), axis=1), -1)\n",
    "            pi_gathered = pi[NX_rep, :, :]\n",
    "\n",
    "            X_softmax = nd.sum(X_softmax * pi_gathered, axis=1)\n",
    "            X_softmax_end = nd.sum(X_softmax_end * pi, axis=1)\n",
    "        else:\n",
    "            X_softmax = squeeze(X_softmax, 1)\n",
    "            X_softmax_end = squeeze(X_softmax_end, 1)\n",
    "\n",
    "        # generate output probabilities\n",
    "        connect, append = X_softmax[:, :self.N_B], X_softmax[:, self.N_B:]\n",
    "        append = append.reshape([-1, self.N_A, self.N_B])\n",
    "        end = squeeze(X_softmax_end, -1)\n",
    "        #print(f\"connect probability shape is: {connect.shape}\")\n",
    "        #print(f\"connect probabilities are: {connect}\")\n",
    "        return append, connect, end\n",
    "\n",
    "\n",
    "class BatchNorm(nn.Block):\n",
    "\n",
    "    def __init__(self, in_channels, momentum=0.9, eps=1e-5):\n",
    "        super(BatchNorm, self).__init__()\n",
    "        self.F = in_channels\n",
    "\n",
    "        self.bn_weight = self.params.get('bn_weight', shape=(self.F,), init=mx.init.One(),\n",
    "                                         allow_deferred_init=False)\n",
    "        self.bn_bias = self.params.get('bn_bias', shape=(self.F,), init=mx.init.Zero(),\n",
    "                                       allow_deferred_init=False)\n",
    "\n",
    "        self.running_mean = self.params.get('running_mean', grad_req='null',\n",
    "                                            shape=(self.F,),\n",
    "                                            init=mx.init.Zero(),\n",
    "                                            allow_deferred_init=False,\n",
    "                                            differentiable=False)\n",
    "        self.running_var = self.params.get('running_var', grad_req='null',\n",
    "                                           shape=(self.F,),\n",
    "                                           init=mx.init.One(),\n",
    "                                           allow_deferred_init=False,\n",
    "                                           differentiable=False)\n",
    "        self.momentum = momentum\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        if autograd.is_training():\n",
    "            return nd.BatchNorm(x,\n",
    "                                gamma=self.bn_weight.data(x.context),\n",
    "                                beta=self.bn_bias.data(x.context),\n",
    "                                moving_mean=self.running_mean.data(x.context),\n",
    "                                moving_var=self.running_var.data(x.context),\n",
    "                                eps=self.eps, momentum=self.momentum,\n",
    "                                use_global_stats=False)\n",
    "        else:\n",
    "            return nd.BatchNorm(x,\n",
    "                                gamma=self.bn_weight.data(x.context),\n",
    "                                beta=self.bn_bias.data(x.context),\n",
    "                                moving_mean=self.running_mean.data(x.context),\n",
    "                                moving_var=self.running_var.data(x.context),\n",
    "                                eps=self.eps, momentum=self.momentum,\n",
    "                                use_global_stats=True)\n",
    "\n",
    "\"\"\"# Building generative network\"\"\"\n",
    "\n",
    "class MoleculeGenerator(nn.Block):\n",
    "\n",
    "    __metaclass__ = ABCMeta\n",
    "\n",
    "    def __init__(self, N_A, N_B, D, F_e, F_skip, F_c, Fh_policy, activation,\n",
    "                 *args, **kwargs):\n",
    "        super(MoleculeGenerator, self).__init__()\n",
    "        self.N_A = N_A                   # no. of unique atom types\n",
    "        self.N_B = N_B                   # no. of unique bond types\n",
    "        self.D = D                       # receptive field\n",
    "        self.F_e = F_e                   # initial embedding layer\n",
    "        self.F_skip = F_skip             # skip connection layer\n",
    "        self.F_c = list(F_c) if isinstance(F_c, tuple) else F_c                   # size of grpah convolution layers\n",
    "        self.Fh_policy = Fh_policy\n",
    "        self.activation = get_activation(activation)\n",
    "\n",
    "        with self.name_scope():\n",
    "            # embeddings\n",
    "            self.embedding_atom = nn.Embedding(self.N_A, self.F_e)\n",
    "            self.embedding_mask = nn.Embedding(3, self.F_e)\n",
    "\n",
    "            # graph conv\n",
    "            self._build_graph_conv(*args, **kwargs)\n",
    "\n",
    "            # fully connected\n",
    "            self.dense = nn.Sequential()\n",
    "            for i, (f_in, f_out) in enumerate(zip([self.F_skip, ] + self.F_c[:-1], self.F_c)):\n",
    "                self.dense.add(Linear_BN(f_in, f_out))\n",
    "\n",
    "            # policy\n",
    "            self.policy_0 = self.params.get('policy_0', shape=[self.N_A, ],\n",
    "                                            init=mx.init.Zero(),\n",
    "                                            allow_deferred_init=False)\n",
    "            self.policy_h = Policy(self.F_c[-1], self.Fh_policy, self.N_A, self.N_B)\n",
    "\n",
    "        self.mode = 'loss'\n",
    "\n",
    "    @abstractmethod\n",
    "    def _build_graph_conv(self, *args, **kwargs):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @abstractmethod\n",
    "    def _graph_conv_forward(self, X, A):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    # for prob score of starting atom\n",
    "    def _policy_0(self, ctx):\n",
    "        policy_0 = nd.exp(self.policy_0.data(ctx))\n",
    "        policy_0 = policy_0/policy_0.sum()\n",
    "        return policy_0\n",
    "\n",
    "    def _policy(self, X, A, NX, NX_rep, last_append_mask):\n",
    "        # get initial embedding\n",
    "        print(f\"We are in policy now\")\n",
    "        print(f\"shape of X is {X.shape} and last_append_mask is {last_append_mask.shape}\")\n",
    "        print(f\"X is {X[0]}\")\n",
    "        X = self.embedding_atom(X) + self.embedding_mask(last_append_mask)\n",
    "        print(f\"shape of X after operation is {X.shape}\")\n",
    "\n",
    "        # convolution\n",
    "        X = self._graph_conv_forward(X, A)\n",
    "\n",
    "        # linear\n",
    "        X = self.dense(X)\n",
    "\n",
    "        # policy\n",
    "        append, connect, end = self.policy_h(X, NX, NX_rep)\n",
    "\n",
    "       \n",
    "\n",
    "        return append, connect, end\n",
    "\n",
    "    def _likelihood(self, init, append, connect, end,\n",
    "                    action_0, actions, iw_ids, log_p_sigma,\n",
    "                    batch_size, iw_size, tox_class_batch):\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # decompose action:\n",
    "        action_type, node_type, edge_type, append_pos, connect_pos = \\\n",
    "            actions[:, 0], actions[:, 1], actions[:, 2], actions[:, 3], actions[:, 4]\n",
    "        _log_mask = lambda _x, _mask: _mask * nd.log(_x + 1e-10) + (1- _mask) * nd.zeros_like(_x)\n",
    "       \n",
    "        init = init.reshape([batch_size * iw_size, self.N_A])\n",
    "      \n",
    "        index = nd.stack(nd.arange(action_0.shape[0], ctx=action_0.context, dtype='int32'), action_0, axis=0)\n",
    "       \n",
    "        loss_init = nd.log(nd.gather_nd(init, index) + 1e-10)\n",
    "\n",
    "        \n",
    "        loss_end = _log_mask(end, nd.cast(action_type == 2, 'float32'))\n",
    "        \n",
    "\n",
    "\n",
    "        # append\n",
    "        index = nd.stack(append_pos, node_type, edge_type, axis=0)\n",
    "        loss_append = _log_mask(nd.gather_nd(append, index), nd.cast(action_type == 0, 'float32'))\n",
    "        \n",
    "\n",
    "        # connect\n",
    "        index = nd.stack(connect_pos, edge_type, axis=0)\n",
    "        loss_connect = _log_mask(nd.gather_nd(connect, index), nd.cast(action_type == 1, 'float32'))\n",
    "        \n",
    "\n",
    "\n",
    "        # sum up results\n",
    "        log_p_x = loss_end + loss_append + loss_connect\n",
    "        log_p_x = squeeze(SegmentSumFn(iw_ids, batch_size*iw_size)(unsqueeze(log_p_x, -1)), -1)\n",
    "        #print(f\"log_p_x shape before transformation is {log_p_x.shape}\")\n",
    "        #log_p_x = unsqueeze(log_p_x, -1)\n",
    "       \n",
    "        log_p_x = log_p_x + loss_init\n",
    "\n",
    "        # reshape\n",
    "        log_p_x = log_p_x.reshape([batch_size, iw_size])\n",
    "        log_p_sigma = log_p_sigma.reshape([batch_size, iw_size])\n",
    "        l = log_p_x - log_p_sigma\n",
    "        l = logsumexp(l, axis=1) - math.log(float(iw_size))\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        #print(f\"Loss is {l}\")\n",
    "        return l\n",
    "\n",
    "    def forward(self, *input):\n",
    "        if self.mode=='loss' or self.mode=='likelihood':\n",
    "            X, A, iw_ids, last_append_mask, \\\n",
    "            NX, NX_rep, action_0, actions, log_p,tox_class_batch, \\\n",
    "            batch_size, iw_size = input\n",
    "\n",
    "            init = self._policy_0(X.context).tile([batch_size * iw_size, 1])\n",
    "            append, connect, end = self._policy(X, A, NX, NX_rep, last_append_mask)\n",
    "            l = self._likelihood(init, append, connect, end, action_0, actions, iw_ids, log_p, batch_size, iw_size, tox_class_batch)\n",
    "            if self.mode=='likelihood':\n",
    "                #print(f\"We are in likelihood mode\")\n",
    "                return l\n",
    "            else:\n",
    "                return -l.mean()\n",
    "        elif self.mode == 'decode_0':\n",
    "            #print(f\"We are in decode_0 mode\")\n",
    "            return self._policy_0(input[0])\n",
    "        elif self.mode == 'decode_step':\n",
    "            #print(f\"We are in decode_step mode\")\n",
    "            X, A, NX, NX_rep, last_append_mask = input\n",
    "            return self._policy(X, A, NX, NX_rep, last_append_mask)\n",
    "\n",
    "\n",
    "class MoleculeGenerator_RNN(MoleculeGenerator):\n",
    "\n",
    "    __metaclass__ = ABCMeta\n",
    "\n",
    "    def __init__(self, N_A, N_B, D, F_e, F_skip, F_c, Fh_policy, activation,\n",
    "                 N_rnn, *args, **kwargs):\n",
    "        super(MoleculeGenerator_RNN, self).__init__(N_A, N_B, D, F_e, F_skip, F_c, Fh_policy, activation,\n",
    "                                                    *args, **kwargs)\n",
    "        self.N_rnn = N_rnn\n",
    "\n",
    "        with self.name_scope():\n",
    "            self.rnn = gluon.rnn.GRU(hidden_size=self.F_c[-1],\n",
    "                                     num_layers=self.N_rnn,\n",
    "                                     layout='NTC', input_size=self.F_c[-1] * 2)\n",
    "\n",
    "    def _rnn_train(self, X, NX, NX_rep, graph_to_rnn, rnn_to_graph, NX_cum):\n",
    "        X_avg = SegmentSumFn(NX_rep, NX.shape[0])(X) / nd.cast(unsqueeze(NX, 1), 'float32')\n",
    "        X_curr = nd.take(X, indices=NX_cum-1)\n",
    "        X = nd.concat(X_avg, X_curr, dim=1)\n",
    "\n",
    "        # rnn\n",
    "        X = nd.take(X, indices=graph_to_rnn) # batch_size, iw_size, length, num_features\n",
    "        batch_size, iw_size, length, num_features = X.shape\n",
    "        X = X.reshape([batch_size*iw_size, length, num_features])\n",
    "        X = self.rnn(X)\n",
    "\n",
    "        X = X.reshape([batch_size, iw_size, length, -1])\n",
    "        X = nd.gather_nd(X, indices=rnn_to_graph)\n",
    "\n",
    "        return X\n",
    "\n",
    "    def _rnn_test(self, X, NX, NX_rep, NX_cum, h):\n",
    "        # note: one partition for one molecule\n",
    "        X_avg = SegmentSumFn(NX_rep, NX.shape[0])(X) / nd.cast(unsqueeze(NX, 1), 'float32')\n",
    "        X_curr = nd.take(X, indices=NX_cum - 1)\n",
    "        X = nd.concat(X_avg, X_curr, dim=1) # size: [NX, F_in * 2]\n",
    "\n",
    "        # rnn\n",
    "        X = unsqueeze(X, axis=1)\n",
    "        X, h = self.rnn(X, h)\n",
    "\n",
    "        X = squeeze(X, axis=1)\n",
    "        return X, h\n",
    "\n",
    "    def _policy(self, X, A, NX, NX_rep, last_append_mask, graph_to_rnn, rnn_to_graph, NX_cum):\n",
    "        # get initial embedding\n",
    "        # X = self.embedding_atom(X) + self.embedding_mask(last_append_mask)\n",
    "\n",
    "        \n",
    "        X = self.embedding_atom(X) + self.embedding_mask(last_append_mask)\n",
    "\n",
    "        # convolution\n",
    "        X = self._graph_conv_forward(X, A)\n",
    "\n",
    "        # linear\n",
    "        X = self.dense(X)\n",
    "\n",
    "        # rnn\n",
    "        X_mol = self._rnn_train(X, NX, NX_rep, graph_to_rnn, rnn_to_graph, NX_cum)\n",
    "\n",
    "        # policy\n",
    "        append, connect, end = self.policy_h(X, NX, NX_rep, X_mol)\n",
    "        #print(f\"end probability is: {end}\")\n",
    "       # print(f\"Length of end_probability is {len(end)}\")\n",
    "        return append, connect, end\n",
    "\n",
    "        \n",
    "\n",
    "    def _decode_step(self, X, A, NX, NX_rep, last_append_mask, NX_cum, h):\n",
    "        # get initial embedding\n",
    "        X = self.embedding_atom(X) + self.embedding_mask(last_append_mask)\n",
    "\n",
    "        # convolution\n",
    "        X = self._graph_conv_forward(X, A)\n",
    "\n",
    "        # linear\n",
    "        X = self.dense(X)\n",
    "\n",
    "        # rnn\n",
    "        X_mol, h = self._rnn_test(X, NX, NX_rep, NX_cum, h)\n",
    "\n",
    "        # policy\n",
    "        append, connect, end = self.policy_h(X, NX, NX_rep, X_mol)\n",
    "        \n",
    "\n",
    "        return append, connect, end, h\n",
    "\n",
    "    def forward(self, *input):\n",
    "        if self.mode=='loss' or self.mode=='likelihood':\n",
    "            X, A, iw_ids, last_append_mask, \\\n",
    "            NX, NX_rep, action_0, actions, log_p, \\\n",
    "            batch_size, iw_size, tox_class, \\\n",
    "            graph_to_rnn, rnn_to_graph, NX_cum = input\n",
    "\n",
    "            init = self._policy_0(X.context).tile([batch_size * iw_size, 1])\n",
    "            append, connect, end = self._policy(X, A, NX, NX_rep, last_append_mask, graph_to_rnn, rnn_to_graph, NX_cum)\n",
    "            l = self._likelihood(init, append, connect, end, action_0, actions, iw_ids, log_p, batch_size, iw_size, tox_class)\n",
    "            if self.mode=='likelihood':\n",
    "               # print(f\"We are in likelihood mode\")\n",
    "                return l\n",
    "            else:\n",
    "                return -l.mean()\n",
    "        elif self.mode == 'decode_0':\n",
    "           # print(f\"We are in decode_0 mode\")\n",
    "            return self._policy_0(input[0])\n",
    "        elif self.mode == 'decode_step':\n",
    "           # print(f\"We are in decode_step mode\")\n",
    "            X, A, NX, NX_rep, last_append_mask, NX_cum, h = input\n",
    "            return self._decode_step(X, A, NX, NX_rep, last_append_mask, NX_cum, h)\n",
    "        else:\n",
    "            raise ValueError\n",
    "\n",
    "class VanillaMolGen_RNN(MoleculeGenerator_RNN):\n",
    "\n",
    "    def __init__(self, N_A, N_B, D, F_e, F_h, F_skip, F_c, Fh_policy, activation, N_rnn):\n",
    "        super(VanillaMolGen_RNN, self).__init__(N_A, N_B, D, F_e, F_skip, F_c, Fh_policy, activation, N_rnn,\n",
    "                                                F_h)\n",
    "\n",
    "    def _build_graph_conv(self, F_h):\n",
    "        self.F_h = list(F_h) if isinstance(F_h, tuple) else F_h\n",
    "        self.conv, self.bn = [], []\n",
    "        for i, (f_in, f_out) in enumerate(zip([self.F_e] + self.F_h[:-1], self.F_h)):\n",
    "           # print(f\"We are in graph_conv forward yes\")\n",
    "           # print(f\"f_in is {f_in}, f_out is {f_out},N_B is {self.N_B}, D is {self.D}\")\n",
    "            conv = GraphConv(f_in, f_out, self.N_B + self.D)\n",
    "            self.conv.append(conv)\n",
    "            self.register_child(conv)\n",
    "\n",
    "            if i != 0:\n",
    "                bn = BatchNorm(in_channels=f_in)\n",
    "                self.register_child(bn)\n",
    "            else:\n",
    "                bn = None\n",
    "            self.bn.append(bn)\n",
    "\n",
    "        self.bn_skip = BatchNorm(in_channels=sum(self.F_h))\n",
    "        self.linear_skip = Linear_BN(sum(self.F_h), self.F_skip)\n",
    "\n",
    "    def _graph_conv_forward(self, X, A):\n",
    "       \n",
    "        X_out = [X]\n",
    "        for conv, bn in zip(self.conv, self.bn):\n",
    "            X = X_out[-1]\n",
    "            if bn is not None:\n",
    "                \n",
    "                X_out.append(conv(self.activation(bn(X)), A))\n",
    "                \n",
    "            else:\n",
    "                X_out.append(conv(X, A))\n",
    "                \n",
    "        X_out = nd.concat(*X_out[1:], dim=1)\n",
    "        return self.activation(self.linear_skip(self.activation(self.bn_skip(X_out))))\n",
    "\n",
    "\"\"\"# Loading and preprocessing the dataset(Necessary to load previous checkpoints)\"\"\"\n",
    "\n",
    "if all([os.path.isfile(os.path.join(ckpt_dir, _n)) for _n in ['log.out', 'ckpt.params', 'trainer.status']]):\n",
    "    is_continuous = True\n",
    "else:\n",
    "    is_continuous = False\n",
    "\n",
    "db_train = dataset\n",
    "num_workers=0\n",
    "# get sampler and loader for training set\n",
    "sampler_train = BalancedSampler(cost=[len(l) for l in db_train], batch_size=batch_size)\n",
    "loader_train = MolRNNLoader(db_train, batch_sampler=sampler_train, num_workers=num_workers, k=k, p=p)\n",
    "\n",
    "it_train = iter(loader_train)\n",
    "\n",
    "\n",
    "\"\"\"### Below code is just for initialization,skip if you want to continue from previous checkpoints\n",
    "\n",
    "# Construcing the model and initializing the parameters\n",
    "\"\"\"\n",
    "\n",
    "if not is_continuous:\n",
    "    configs = {'F_e': F_e,\n",
    "               'F_h': F_h,\n",
    "               'F_skip': F_skip,\n",
    "               'F_c': F_c,\n",
    "               'Fh_policy': Fh_policy,\n",
    "               'activation': activation,\n",
    "               'N_rnn': N_rnn}\n",
    "    with open(os.path.join(ckpt_dir, 'configs.json'), 'w') as f:\n",
    "        json.dump(configs, f)\n",
    "else:\n",
    "    with open(os.path.join(ckpt_dir, 'configs.json')) as f:\n",
    "        configs = json.load(f)\n",
    "\n",
    "model = VanillaMolGen_RNN(get_mol_spec().num_atom_types, get_mol_spec().num_bond_types, D=2, **configs)\n",
    "\n",
    "ctx = mx.gpu()\n",
    "if not is_continuous:\n",
    "    model.collect_params().initialize(mx.init.Xavier(), force_reinit=True, ctx=ctx)\n",
    "else:\n",
    "    model.load_parameters(os.path.join(ckpt_dir, 'ckpt.params'), ctx=ctx)\n",
    "\n",
    "# construct optimizer\n",
    "opt = mx.optimizer.Adam(learning_rate=lr, clip_gradient=clip_grad)\n",
    "trainer = gluon.Trainer(model.collect_params(), opt)\n",
    "if is_continuous:\n",
    "    trainer.load_states(os.path.join(ckpt_dir, 'trainer.status'))\n",
    "\n",
    "\"\"\"# Training the model\"\"\"\n",
    "print(\"We are in training part....\")\n",
    "iterations = 480000\n",
    "\n",
    "\n",
    "if not is_continuous:\n",
    "    t0 = time.time()\n",
    "    global_counter = 0\n",
    "    print(f\"Training is starting from scratch....\")\n",
    "else:\n",
    "    with open(os.path.join(ckpt_dir, 'log.out')) as f:\n",
    "        records = f.readlines()\n",
    "        if records[-1] != 'Training finished\\n':\n",
    "            final_record = records[-1]\n",
    "        else:\n",
    "            final_record = records[-2]\n",
    "    count, t_final = int(final_record.split('\\t')[0]), float(final_record.split('\\t')[1])\n",
    "    t0 = time.time() - t_final * 60\n",
    "    global_counter = count\n",
    "    print(f\"Training is continuing from golabal_counter : {global_counter}\")\n",
    "\n",
    "with open(os.path.join(ckpt_dir, 'log.out'),\n",
    "          mode='w' if not is_continuous else 'a') as f:\n",
    "    if not is_continuous:\n",
    "        f.write('step\\ttime(h)\\tloss\\tlr\\n')\n",
    "    while True:\n",
    "        global_counter += 1\n",
    "        \n",
    "\n",
    "        try:\n",
    "            inputs = next(it_train)\n",
    "        except StopIteration:\n",
    "            it_train = iter(loader_train)\n",
    "            inputs = next(it_train)\n",
    "\n",
    "        # move to gpu\n",
    "        inputs = MolRNNLoader.from_numpy_to_tensor(inputs)\n",
    "\n",
    "        with autograd.record():\n",
    "            loss = [(model(*inputs)).as_in_context(mx.gpu())]\n",
    "            loss = sum(loss)\n",
    "            loss.backward()\n",
    "\n",
    "        nd.waitall()\n",
    "        gc.collect()\n",
    "\n",
    "        trainer.step(batch_size=1)\n",
    "        if global_counter % decay_step == 0:\n",
    "            trainer.set_learning_rate(trainer.learning_rate * (1.0 - decay))\n",
    "\n",
    "        if global_counter % summary_step == 0:\n",
    "            loss = (sum(loss)).asnumpy().item()\n",
    "\n",
    "            model.save_parameters(os.path.join(ckpt_dir, 'ckpt.params'))\n",
    "            trainer.save_states(os.path.join(ckpt_dir, 'trainer.status'))\n",
    "\n",
    "            print('Iteration', global_counter, 'done!')\n",
    "            f.write('{}\\t{}\\t{}\\t{}\\n'.format(global_counter, float(time.time() - t0)/60, loss, trainer.learning_rate))\n",
    "            try:\n",
    "                    f.flush()\n",
    "            except Exception as flush_error:\n",
    "                    print(f\"Error flushing to file: {flush_error}\")\n",
    "\n",
    "            del loss, inputs\n",
    "            gc.collect()\n",
    "\n",
    "        if global_counter >= iterations:\n",
    "            break\n",
    "\n",
    "    # save before exit\n",
    "    model.save_parameters(os.path.join(ckpt_dir, 'ckpt.params'))\n",
    "    trainer.save_states(os.path.join(ckpt_dir, 'trainer.status'))\n",
    "\n",
    "    f.write('Training finished\\n')\n",
    "\n",
    "\n",
    "num_params = sum(p.data().size for p in model.collect_params().values())\n",
    "print(f\"Number of parameters in the model: {num_params}\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
